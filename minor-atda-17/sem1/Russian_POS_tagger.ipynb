{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологические процессоры для русского языка\n",
    "\n",
    "Существует два основных вида морфологических процессоров: морфологические анализаторы и морфологические словари.\n",
    "Морфологический анализатор определяет часть речи слова по его парадигме. Умеет определять вероятность того или иного разбора. \n",
    "Морфологический словарь определяет часть речи на основе разметки словаря / корпуса. Может возвращать несколько разборов каждого слова, причем для каждого разбора будет определена его частота в размеченном корпусе. \n",
    "* mystem – морфологический анализатор Yandex (https://tech.yandex.ru/mystem/) . Утверждается, что 3-я версия mystem снимает омонимию.\n",
    "    *  pymystem3 –  обертка для Python (https://github.com/Digsolab/pymystem3)\n",
    "* pymorphy2 – морфологический словарь на основе словарей Открытого корпуса (https://pymorphy2.readthedocs.org/en/latest/)\n",
    "\n",
    "Морфологические процессоры чаще всего используются:\n",
    "1. для лемматизации (lemmatization)\n",
    "2. для определения части речи (Part-of-speech tagging, POS tagging)\n",
    "\n",
    "Эти задачи, разумеется, связанные, но вызовы морфологических процессоров различаются.\n",
    "\n",
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "глокай куздра штеко будланул бокра и курдячить бокренок\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "text = u\"Красивая мама красиво мыла раму\"\n",
    "text = 'Глокая куздра штеко будланула бокра и курдячит бокренка'\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "глокать\n",
      "куздра\n",
      "штёкий\n",
      "будланула\n",
      "бокра\n",
      "и\n",
      "курдячать\n",
      "бокрёнок\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "#возьмем 0 разбор – самый частый в ОК \n",
    "\n",
    "for word in text.split():\n",
    "    print(morph.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='раму', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,datv'), normal_form='рам', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 32, 2),))\n",
      "Parse(word='раму', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='рама', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 55, 3),))\n"
     ]
    }
   ],
   "source": [
    "for i in morph.parse(u'раму'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "глокать\n",
      "куздра\n",
      "штёкий\n",
      "будланула\n",
      "бокра\n",
      "и\n",
      "курдячать\n",
      "бокрёнок\n",
      "рам СУЩ,неод,мр,гео ед,дт\n",
      "рама СУЩ,неод,жр ед,вн\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "#возьмем 0 разбор – самый частый в ОК \n",
    "\n",
    "for word in text.split():\n",
    "    print(morph.parse(word)[0].normal_form)\n",
    "\n",
    "\n",
    "# print morph.parse(u'стекло')\n",
    "# print len(morph.parse(u'стекло'))\n",
    "for i in morph.parse(u'раму'):\n",
    "    print(i.normal_form, i.tag.cyr_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как обстоят дела с омонимией. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "он видеть их семья свой глаз\n",
      "\n",
      "***\n",
      "он\n",
      "видеть\n",
      "они\n",
      "семь\n",
      "свой\n",
      "глаз\n"
     ]
    }
   ],
   "source": [
    "text = u\"Он видел их семью своими глазами\"\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))\n",
    "print('***')\n",
    "for word in text.split():\n",
    "    print(morph.parse(word)[0].normal_form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full info: [{\"analysis\": [{\"gr\": \"A=им,ед,полн,жен\", \"lex\": \"красивый\"}], \"text\": \"Красивая\"}, {\"text\": \" \"}, {\"analysis\": [{\"gr\": \"S,жен,од=им,ед\", \"lex\": \"мама\"}], \"text\": \"мама\"}, {\"text\": \" \"}, {\"analysis\": [{\"gr\": \"ADV=\", \"lex\": \"красиво\"}], \"text\": \"красиво\"}, {\"text\": \" \"}, {\"analysis\": [{\"gr\": \"V,несов,пе=прош,ед,изъяв,жен\", \"lex\": \"мыть\"}], \"text\": \"мыла\"}, {\"text\": \" \"}, {\"analysis\": [{\"gr\": \"S,жен,неод=вин,ед\", \"lex\": \"рама\"}], \"text\": \"раму\"}, {\"text\": \"\\n\"}]\n",
      "Красивая\n",
      "Gram info: A=им,ед,полн,жен\n",
      "POS tag: A\n",
      "мама\n",
      "Gram info: S,жен,од=им,ед\n",
      "POS tag: S\n",
      "красиво\n",
      "Gram info: ADV=\n",
      "POS tag: ADV\n",
      "мыла\n",
      "Gram info: V,несов,пе=прош,ед,изъяв,жен\n",
      "POS tag: V\n",
      "раму\n",
      "Gram info: S,жен,неод=вин,ед\n",
      "POS tag: S\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "text = u\"Красивая мама красиво мыла раму\"\n",
    "gram_info = m.analyze(text)\n",
    "print(\"full info:\", json.dumps(gram_info, ensure_ascii=False))\n",
    "for i in range(len(gram_info)):\n",
    "    if not gram_info[i][\"text\"].isspace():\n",
    "        print(gram_info[i][\"text\"])\n",
    "        print('Gram info:', gram_info[i][\"analysis\"][0][\"gr\"])\n",
    "        print('POS tag:', gram_info[i][\"analysis\"][0][\"gr\"].split(',')[0].split('=')[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Красивая\n",
      "Gram info: ADJF,Qual femn,sing,nomn\n",
      "POS tag: ADJF\n",
      "мама\n",
      "Gram info: NOUN,anim,femn sing,nomn\n",
      "POS tag: NOUN\n",
      "красиво\n",
      "Gram info: ADVB\n",
      "POS tag: ADVB\n",
      "мыла\n",
      "Gram info: NOUN,inan,neut sing,gent\n",
      "POS tag: NOUN\n",
      "раму\n",
      "Gram info: NOUN,inan,masc,Geox sing,datv\n",
      "POS tag: NOUN\n"
     ]
    }
   ],
   "source": [
    "for word in text.split():\n",
    "    print(word)\n",
    "    print('Gram info:', morph.parse(word)[0].tag)\n",
    "    print('POS tag:', morph.parse(word)[0].tag.POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтаксис pymystem3 существенно сложнее, POS тэги не хранятся в явном виде. Для того, что бы узнать POS тэг приходится использовать чудовищную строчку gram_info[i][\"analysis\"][0][\"gr\"].split(',')[0].split('=')[0] . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'analysis': [{'gr': 'A=им,ед,полн,жен', 'lex': 'красивый'}],\n",
       "   'text': 'Красивая'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'gr': 'S,жен,од=им,ед', 'lex': 'мама'}], 'text': 'мама'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'gr': 'ADV=', 'lex': 'красиво'}], 'text': 'красиво'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'gr': 'V,несов,пе=прош,ед,изъяв,жен', 'lex': 'мыть'}],\n",
       "   'text': 'мыла'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'gr': 'S,жен,неод=вин,ед', 'lex': 'рама'}], 'text': 'раму'},\n",
       "  {'text': '\\n'}],\n",
       " 'A')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "gram_info = m.analyze(text)\n",
    "gram_info,gram_info[0][\"analysis\"][0][\"gr\"].split(',')[0].split('=')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как тебя зовут? Катя\n",
      "Катя, где ты была всю прошлую ночь?\n"
     ]
    }
   ],
   "source": [
    "verb = u'быть'\n",
    "verb_parse = morph.parse(verb)[0]\n",
    "femn_gram = {'VERB','impf','femn','sing','past','indc'}\n",
    "musc_gram = {'VERB','impf','musc','sing','past','indc'}\n",
    "\n",
    "femn_name = [u'Катя',u'Оля',u'Юля']\n",
    "musc_name = [u'Митя',u'Сережа',u'Андрей']\n",
    "\n",
    "\n",
    "name = input(u'Привет, как тебя зовут? ')\n",
    "if name in femn_name:\n",
    "    print(u'%s, где ты %s всю прошлую ночь?' %(name,verb_parse.inflect(femn_gram).word))\n",
    "elif name in musc_name:\n",
    "    print(u'%s, где ты %s всю прошлую ночь?' %(name,verb_parse.inflect(musc_gram).word))\n",
    "else:\n",
    "    print(u'%s, как прошла ночь?' %(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "1. Записать произвольный текст в файл.\n",
    "2. Считать текст из файла.\n",
    "3. Используя оба морфологических процессора, найти все существительные в тексте. На выходе должно быть 2 независимых списка существительных.\n",
    "4. Посчитать количество совпадений и различий.\n",
    "5. Используя nltk.FreqDist(), посчитать частоты всех существительных. На выходе должно быть 2 независимых частотных словаря. \n",
    "6. Найти 10 самых частых существительных в каждом частотном словаре (nltk.FreqDist().most_common()) и вывести их в цикле.\n",
    "7. Можно ли эти существительные считать *ключевыми словами*? \n",
    "\n",
    "## Задание 2\n",
    "1. Написать программу, которая ведет осмысленный диалог и склоняет существительные и глаголы. \n",
    "\n",
    "## Задание 3\n",
    "1. Найти два текста разных стилей (или жанров), например, описание продукта из интернет-магазина и инструкцию по его употреблению.\n",
    "2. Использовать любой морфологический процессор для разбора обоих текстов. На выходе должно быть два разобранных текста: каждый текст представлен списком из списков, каждый мини-список – это пара [слово, POS тег]. \n",
    "3. Посчитать, сколько раз какие части речи употребляются в каждом тексте (можно использовать nltk.FreqDist()).\n",
    "4. Можно ли на основе частот разных частей речи делать выводы о стилевых различиях между текстами?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература\n",
    "\n",
    "* Manning & Shuetze: Ch. 10 – Part of Speech Tagging\n",
    "\n",
    "* NLTK Book: Ch. 1, 3.1   Frequency Distributions – частотные словари\n",
    "\n",
    "* Статья Сегаловича про mystem – http://download.yandex.ru/company/iseg-las-vegas.pdf\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "Russian_POS_tagger.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
