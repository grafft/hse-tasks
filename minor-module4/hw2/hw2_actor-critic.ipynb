{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ2 - алгоритм актор-критик\n",
    "Крайний срок сдачи - 24.03.2019 23:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Задача будет заключаться в том, чтобы провести эксперименты с алгоритмом актор-критик, который мы подробно разбирали на семинарах. Ваше решение в виде Jupyter тетрадки *с комментариями* нужно загрузить по [ссылке](https://www.dropbox.com/request/nBSHbnUTG9sLnEexYSW2) с шаблоном имени \"Фамилия>\\_ИАД<номер группы>.ipynb\", например Панов_ИАД1.ipynb. Указывать ту группу, которую вы посещаете на семинарах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Требования:\n",
    "### 1. __TensorFlow:__\n",
    "Инструкции-напоминания по установке здесь - https://www.tensorflow.org/get_started/os_setup. GPU не будет требоваться для выполнения ДЗ.\n",
    "### 2. __OpenAI Gym:__  \n",
    "Напоминание-инструкции по OpenAI Gym - https://gym.openai.com/docs. Используйте версию 0.10.5.\n",
    "### 3. __TensorFlow-probabilities:__<br>\n",
    "Инструкции по установке - https://www.tensorflow.org/probability/install. Возможно, вам понадобится обновить TensorFlow и numpy. Еще нам файл logz.py ля полезных функций ведения лога процесса обучения.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все необходимое и пишем вспомогательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "print(gym.__version__)\n",
    "\n",
    "import logz\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from multiprocessing import Process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def setup_logger(logdir, locals_):\n",
    "    # Configure output directory for logging\n",
    "    logz.configure_output_dir(logdir)\n",
    "    # Log experimental parameters\n",
    "    args = inspect.getargspec(train_PG)[0]\n",
    "    params = {k: locals_[k] if k in locals_ else None for k in args}\n",
    "    logz.save_params(params)\n",
    "    \n",
    "x=[]\n",
    "y=[]\n",
    "fig=None\n",
    "def plot(t,mean_r):\n",
    "    if t==0:\n",
    "        fig=plt.figure()\n",
    "    x.append(t)\n",
    "    y.append(mean_r)\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(x,y,label='mean_reward')\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Напоминаем необходимые формулы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминаем алгоритм актор-критика:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Alonso94/homework/master/hw3/ac.png\"/>\n",
    "\n",
    "Вычслять градиент целевой функции для определения градиент стратегии мы будем по следующей формуле: \n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{T} \\nabla_\\theta\\log\\pi_\\theta(a_{it},s_{it}) A^\\pi(s_t,a_t).$$\n",
    "\n",
    "Здесь мы оцениваем функцию полезности действия через функцию преимущества $A$, которую для уменьшения дисперсии решения будем представлять в виде разницы оценки критика  и базовой функции $B(s)=V_v^\\pi(s)$:\n",
    "\n",
    "$$A^\\pi(s_t,a_t)\\approx r(a_t,s_t)+\\gamma V_v^\\pi(s_{t+1})-V_v^\\pi(s_{it})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Критик, который оценивает функцию преимущества минимизует среднеквадратичную ошибку для TD-показателя $y_t=r(a_t,s_t)+\\gamma V^\\pi(s_{t+1})$:\n",
    "\n",
    "$$\\min_v \\sum_{i,t}(V_v^\\pi(s_{it})-y_{it})^2.$$\n",
    "\n",
    "Для того, чтобы ускорить работу мы будем повторять слеюущие два шага:\n",
    "\n",
    "1. Обновлять показатель со старым значеним функции полезности.\n",
    "2. Делаем несколько градиентных шагов для обновления функции полезности.\n",
    "\n",
    "Т.е. мы обучение критика идет интерстивно: обновляем критерий и затем обновляем саму функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Создаем аппроксиматор (2 балла).\n",
    "\n",
    "Нам нужно создать простой аппроксиматор в виде нескольких слоев (__n_layers__) полносвязных нейронных сетей (__tf.layers.dense__). Большиснтвоа параметров передается вам в качестве параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_placeholder, output_size, scope, n_layers, size, activation=tf.tanh, output_activation=None):\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "        \n",
    "        arguments:\n",
    "            input_placeholder: placeholder variable for the state (batch_size, input_size)\n",
    "            output_size: size of the output layer\n",
    "            scope: variable scope of the network\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of the hidden layer\n",
    "            activation: activation of the hidden layers\n",
    "            output_activation: activation of the ouput layers\n",
    "        returns:\n",
    "            output placeholder of the network (the result of a forward pass) \n",
    "        Hint: use tf.layers.dense    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # Ваш код здесь\n",
    "    \n",
    "    \n",
    "    return output_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы будем реализовывать основной класс Agent. Вначале определим его конструктор, параметры которого будем брать из словаря __computation_graph_args__. Параметры __computation_graph_args__ - вспомогательные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, computation_graph_args, sample_trajectory_args, estimate_advantage_args):\n",
    "        super(Agent, self).__init__()\n",
    "        self.ob_dim = computation_graph_args['ob_dim']\n",
    "        self.ac_dim = computation_graph_args['ac_dim']\n",
    "        self.discrete = computation_graph_args['discrete']\n",
    "        self.size = computation_graph_args['size']\n",
    "        self.n_layers = computation_graph_args['n_layers']\n",
    "        self.learning_rate = computation_graph_args['learning_rate']\n",
    "        self.num_target_updates = computation_graph_args['num_target_updates']\n",
    "        self.num_grad_steps_per_target_update = computation_graph_args['num_grad_steps_per_target_update']\n",
    "\n",
    "        self.animate = sample_trajectory_args['animate']\n",
    "        self.max_path_length = sample_trajectory_args['max_path_length']\n",
    "        self.min_timesteps_per_batch = sample_trajectory_args['min_timesteps_per_batch']\n",
    "\n",
    "        self.gamma = estimate_advantage_args['gamma']\n",
    "        self.normalize_advantages = estimate_advantage_args['normalize_advantages']\n",
    "\n",
    "    def init_tf_sess(self):\n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        tf_config.gpu_options.allow_growth = True # may need if using GPU\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__() # equivalent to `with self.sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "            Placeholders for batch batch observations / actions / advantages in actor critic\n",
    "            loss function.\n",
    "            See Agent.build_computation_graph for notation\n",
    "\n",
    "            returns:\n",
    "                sy_ob_no: placeholder for observations\n",
    "                sy_ac_na: placeholder for actions\n",
    "                sy_adv_n: placeholder for advantages\n",
    "        \"\"\"\n",
    "        sy_ob_no = tf.placeholder(shape=[None, self.ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "        if self.discrete:\n",
    "            sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32) \n",
    "        else:\n",
    "            sy_ac_na = tf.placeholder(shape=[None, self.ac_dim], name=\"ac\", dtype=tf.float32) \n",
    "        \n",
    "        sy_adv_n = tf.placeholder(shape=[None],name=\"advantage\",dtype=tf.float32)\n",
    "        return sy_ob_no, sy_ac_na, sy_adv_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Задание 2. Реализуем алгоритм работы актора\n",
    "\n",
    "Во-первых нам нужно дописать код для определения действия по текущему наблюдению (__sy_ob_no__). Эта операция будет различаться для непрерывных и дискретных окружений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward_pass(self, sy_ob_no):\n",
    "    \"\"\" Constructs the symbolic operation for the policy network outputs,\n",
    "        which are the parameters of the policy distribution p(a|s)\n",
    "        arguments:\n",
    "            sy_ob_no: (batch_size, self.ob_dim)\n",
    "        returns:\n",
    "            the parameters of the policy.\n",
    "            if discrete, the parameters are the logits of a categorical distribution\n",
    "                over the actions\n",
    "                sy_logits_na: (batch_size, self.ac_dim)\n",
    "            if continuous, the parameters are a tuple (mean, log_std) of a Gaussian\n",
    "                distribution over actions. log_std should just be a trainable\n",
    "                variable, not a network output.\n",
    "                sy_mean: (batch_size, self.ac_dim)\n",
    "                sy_logstd: (self.ac_dim,)\n",
    "        Hint: use the 'build_mlp' function to output the logits (in the discrete case)\n",
    "            and the mean (in the continuous case).\n",
    "            Pass in self.n_layers for the 'n_layers' argument, and\n",
    "            pass in self.size for the 'size' argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    scope=\"nn_policy\"\n",
    "    if self.discrete:\n",
    "        sy_logits_na = build_mlp(sy_ob_no,self.ac_dim,scope,self.n_layers,self.size, activation=tf.nn.relu)\n",
    "        return sy_logits_na\n",
    "    else:\n",
    "        sy_mean = build_mlp(sy_ob_no,self.ac_dim,scope,self.n_layers,self.size,activation=tf.nn.relu)\n",
    "        sy_logstd = tf.Variable(np.zeros(self.ac_dim),name=\"sy_logstd\")\n",
    "        return (sy_mean, sy_logstd)\n",
    "    \n",
    "Agent.policy_forward_pass = policy_forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы выбираем нужное действие из распределения, которое задается нам стратегией. Для дискретного и непрерывного случая используем разные методы (__tf.squeeze__ и __tf.random_normal__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(self, policy_parameters):\n",
    "    \"\"\" Constructs a symbolic operation for stochastically sampling from the policy\n",
    "        distribution\n",
    "        arguments:\n",
    "            policy_parameters\n",
    "                if discrete: logits of a categorical distribution over actions \n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "        returns:\n",
    "            sy_sampled_ac: \n",
    "                if discrete: (batch_size,)\n",
    "                if continuous: (batch_size, self.ac_dim)\n",
    "        Hint: for the continuous case, use the reparameterization trick:\n",
    "             The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "\n",
    "                  mu + sigma * z,         z ~ N(0, I)\n",
    "\n",
    "             This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "    \"\"\"\n",
    "    if self.discrete:\n",
    "        sy_logits_na = policy_parameters\n",
    "        # use tf.squeeze (Removes dimensions of size 1 from the shape of a tensor.)\n",
    "        sy_sampled_ac = tf.squeeze(tf.multinomial(sy_logits_na, 1), axis=1)\n",
    "    else:\n",
    "        sy_mean, sy_logstd = policy_parameters\n",
    "        exp=tf.cast(tf.exp(sy_logstd), dtype=tf.float32)\n",
    "        sy_sampled_ac = sy_mean + exp * tf.random_normal(shape=tf.shape(sy_mean))\n",
    "    return sy_sampled_ac\n",
    "\n",
    "Agent.sample_action = sample_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы вычисляем логарифм вероятности действия для использования в целевой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(self, policy_parameters, sy_ac_na):\n",
    "    \"\"\" Constructs a symbolic operation for computing the log probability of a set of actions\n",
    "        that were actually taken according to the policy\n",
    "        arguments:\n",
    "            policy_parameters\n",
    "                if discrete: logits of a categorical distribution over actions \n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "            sy_ac_na: \n",
    "                if discrete: (batch_size,)\n",
    "                if continuous: (batch_size, self.ac_dim)\n",
    "        returns:\n",
    "            sy_logprob_n: (batch_size)\n",
    "        Hint:\n",
    "            For the discrete case, use the log probability under a categorical distribution.\n",
    "            For the continuous case, use the log probability under a multivariate gaussian.\n",
    "    \"\"\"\n",
    "    if self.discrete:\n",
    "        sy_logits_na = policy_parameters\n",
    "        # use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        sy_logprob_n = - tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sy_ac_na,logits=sy_logits_na)\n",
    "    else:\n",
    "        sy_mean, sy_logstd = policy_parameters\n",
    "        # multivariate gaussian (tf.distributions.Normal)\n",
    "        exp=tf.cast(tf.exp(sy_logstd),dtype=tf.float32)\n",
    "        probabilities = tf.distributions.Normal(sy_mean, exp).prob(sy_ac_na)\n",
    "        sy_logprob_n = tf.log(tf.reduce_prod(probabilities, axis=1))\n",
    "    return sy_logprob_n\n",
    "\n",
    "Agent.get_log_prob = get_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, записываем код обновления параметров актора в сессии. Здесь __actor_update_op__ - оптимизатор, который определим позже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor(self, ob_no, ac_na, adv_n):\n",
    "    \"\"\" \n",
    "        Update the parameters of the policy.\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            ac_na: shape: (sum_of_path_lengths).\n",
    "            adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                advantages whose length is the sum of the lengths of the paths\n",
    "\n",
    "        returns:\n",
    "            nothing\n",
    "\n",
    "    \"\"\"\n",
    "    self.sess.run(self.actor_update_op,feed_dict={self.sy_ob_no: ob_no, self.sy_ac_na: ac_na, self.sy_adv_n: adv_n})\n",
    "    \n",
    "Agent.update_actor = update_actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Задание 3. Пишем код для критика (4 балла)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с оценки функции преимущества (см. формулы в начале тетрадки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantage(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "    \"\"\"\n",
    "        Estimates the advantage function value for each timestep.\n",
    "\n",
    "        let sum_of_path_lengths be the sum of the lengths of the paths sampled from \n",
    "            Agent.sample_trajectories\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "            re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                the reward for each timestep\n",
    "            terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                at that timestep of 0 if the episode did not end\n",
    "\n",
    "        returns:\n",
    "            adv_n: shape: (sum_of_path_lengths). A single vector for the estimated \n",
    "                advantages whose length is the sum of the lengths of the paths\n",
    "    \"\"\"\n",
    "    # First, estimate the Q value as Q(s, a) = r(s, a) + gamma*V(s')\n",
    "    # To get the advantage, subtract the V(s) to get A(s, a) = Q(s, a) - V(s)\n",
    "    # This requires calling the critic twice --- to obtain V(s') when calculating Q(s, a),\n",
    "    # and V(s) when subtracting the baseline\n",
    "    # Note: don't forget to use terminal_n to cut off the V(s') term when computing Q(s, a)\n",
    "    # otherwise the values will grow without bound.\n",
    "\n",
    "    adv_n = []\n",
    "    # Ваш код здесь\n",
    "    # Считаем значение полезност следующего состояния s' через критика\n",
    "    v_s_tp1_n = None\n",
    "    # Считаем значение полезност текущего состояния s через критика\n",
    "    v_s_t_n = None\n",
    "    # Считаем значение Q(s, a) = r(s, a) + gamma*V(s')\n",
    "    q_n = None\n",
    "    # Считаем A(s, a) = Q(s, a) - V(s)\n",
    "    adv_n = None        \n",
    "\n",
    "    if self.normalize_advantages:\n",
    "        # Ваш код здесь\n",
    "        # Вчитаем среднее и делим на дисперсию\n",
    "        adv_n = None\n",
    "    return adv_n\n",
    "\n",
    "Agent.estimate_advantage = estimate_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь неободимо записать то, как мы обновляем параметры критика в сессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "    \"\"\"\n",
    "        Update the parameters of the critic.\n",
    "\n",
    "        let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "            Agent.sample_trajectories\n",
    "        let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "            re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                the reward for each timestep\n",
    "            terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                at that timestep of 0 if the episode did not end\n",
    "\n",
    "        returns:\n",
    "            nothing\n",
    "    \"\"\"\n",
    "    # Use a bootstrapped target values to update the critic\n",
    "    # Compute the target values r(s, a) + gamma*V(s') by calling the critic to compute V(s')\n",
    "    # In total, take n=self.num_grad_steps_per_target_update*self.num_target_updates gradient update steps\n",
    "    # Every self.num_grad_steps_per_target_update steps, recompute the target values\n",
    "    # by evaluating V(s') on the updated critic\n",
    "    # Note: don't forget to use terminal_n to cut off the V(s') term when computing the target\n",
    "    # otherwise the values will grow without bound.\n",
    "\n",
    "    for i in range(self.num_target_updates):\n",
    "        target_n = []\n",
    "        # Ваш код здесь\n",
    "        # Считаем значение полезност следующего состояния s' через критика\n",
    "        v_s_tp1_n = None\n",
    "        # Считаем значение показателя временных различий r(s, a) + gamma*V(s') если это не терминальное состояние\n",
    "        target_n = None\n",
    "\n",
    "        for j in range(self.num_grad_steps_per_target_update):\n",
    "            self.sess.run(self.critic_update_op, feed_dict={self.sy_target_n: target_n,self.sy_ob_no: ob_no})\n",
    "            \n",
    "Agent.update_critic = update_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Задание 4. Собираем все вместе.\n",
    "\n",
    "Теперь нам осталось собрать все предыдущие методы в один граф вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_computation_graph(self):\n",
    "    \"\"\"\n",
    "        Notes on notation:\n",
    "\n",
    "        Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "        that are computed later in the function\n",
    "\n",
    "        Prefixes and suffixes:\n",
    "        ob - observation \n",
    "        ac - action\n",
    "        _no - this tensor should have shape (batch self.size /n/, observation dim)\n",
    "        _na - this tensor should have shape (batch self.size /n/, action dim)\n",
    "        _n  - this tensor should have shape (batch self.size /n/)\n",
    "\n",
    "        Note: batch self.size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "        is None\n",
    "\n",
    "        ----------------------------------------------------------------------------------\n",
    "        loss: a function of self.sy_logprob_n and self.sy_adv_n that we will differentiate\n",
    "            to get the policy gradient.\n",
    "    \"\"\"\n",
    "    self.sy_ob_no, self.sy_ac_na, self.sy_adv_n = self.define_placeholders()\n",
    "\n",
    "    # The policy takes in an observation and produces a distribution over the action space\n",
    "    self.policy_parameters = self.policy_forward_pass(self.sy_ob_no)\n",
    "\n",
    "    # We can sample actions from this action distribution.\n",
    "    # This will be called in Agent.sample_trajectory() where we generate a rollout.\n",
    "    self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "\n",
    "    # We can also compute the logprob of the actions that were actually taken by the policy\n",
    "    # This is used in the loss function.\n",
    "    self.sy_logprob_n = self.get_log_prob(self.policy_parameters, self.sy_ac_na)\n",
    "\n",
    "    actor_loss = tf.reduce_sum(-self.sy_logprob_n * self.sy_adv_n)\n",
    "    self.actor_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(actor_loss)\n",
    "\n",
    "    # define the critic\n",
    "    self.critic_prediction = tf.squeeze(build_mlp(\n",
    "                            self.sy_ob_no,\n",
    "                            1,\n",
    "                            \"nn_critic\",\n",
    "                            n_layers=self.n_layers,\n",
    "                            size=self.size))\n",
    "    self.sy_target_n = tf.placeholder(shape=[None], name=\"critic_target\", dtype=tf.float32)\n",
    "    self.critic_loss = tf.losses.mean_squared_error(self.sy_target_n, self.critic_prediction)\n",
    "    self.critic_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "\n",
    "Agent.build_computation_graph = build_computation_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим к агенту еще пару методов работы со статистикой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(self, itr, env):\n",
    "    # Collect paths until we have enough timesteps\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while True:\n",
    "        animate_this_episode=(len(paths)==0 and (itr % 10 == 0) and self.animate)\n",
    "        path = self.sample_trajectory(env, animate_this_episode)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += pathlength(path)\n",
    "        if timesteps_this_batch > self.min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "def sample_trajectory(self, env, animate_this_episode):\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if animate_this_episode:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "        obs.append(ob)\n",
    "        ac = self.sess.run(self.sy_sampled_ac, feed_dict={self.sy_ob_no: ob[None, :]}) # YOUR HW2 CODE HERE\n",
    "        ac = ac[0]\n",
    "        acs.append(ac)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        # add the observation after taking a step to next_obs\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "        steps += 1\n",
    "        # If the episode ended, the corresponding terminal value is 1\n",
    "        # otherwise, it is 0\n",
    "        if done or steps > self.max_path_length:\n",
    "            terminals.append(1)\n",
    "            break\n",
    "        else:\n",
    "            terminals.append(0)\n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "            \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "    return path\n",
    "\n",
    "Agent.sample_trajectories = sample_trajectories\n",
    "Agent.sample_trajectory = sample_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Задание 5. Проводим экспермиенты (4 баллов).\n",
    "\n",
    "Ниже записан код, в котором мы проводим эксперименты с построенным агентом. Вначале мы инициализируем окружение (Set Up Env), затем создаем агента (Initialize Agent) и запускаем его в среду в цикле (Training Loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AC(\n",
    "        exp_name,\n",
    "        env_name,\n",
    "        n_iter=100, \n",
    "        gamma=1, \n",
    "        min_timesteps_per_batch=1000, \n",
    "        max_path_length=None,\n",
    "        learning_rate=5e-3,\n",
    "        num_target_updates=10,\n",
    "        num_grad_steps_per_target_update=10,\n",
    "        animate=True, \n",
    "        normalize_advantages=True,\n",
    "        seed=1,\n",
    "        n_layers=2,\n",
    "        size=64):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Set Up Env\n",
    "    #========================================================================================#\n",
    "\n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Set random seeds\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Maximum length for episodes\n",
    "    max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "    # Is this env continuous, or self.discrete?\n",
    "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "    # Observation and action sizes\n",
    "    ob_dim = env.observation_space.shape[0]\n",
    "    ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Initialize Agent\n",
    "    #========================================================================================#\n",
    "\n",
    "    computation_graph_args = {\n",
    "        'n_layers': n_layers,\n",
    "        'ob_dim': ob_dim,\n",
    "        'ac_dim': ac_dim,\n",
    "        'discrete': discrete,\n",
    "        'size': size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_target_updates': num_target_updates,\n",
    "        'num_grad_steps_per_target_update': num_grad_steps_per_target_update,\n",
    "        }\n",
    "\n",
    "    sample_trajectory_args = {\n",
    "        'animate': animate,\n",
    "        'max_path_length': max_path_length,\n",
    "        'min_timesteps_per_batch': min_timesteps_per_batch,\n",
    "        }\n",
    "\n",
    "    estimate_advantage_args = {\n",
    "        'gamma': gamma,\n",
    "        'normalize_advantages': normalize_advantages,\n",
    "        }\n",
    "    \n",
    "    agent = Agent(computation_graph_args, sample_trajectory_args, estimate_advantage_args) #estimate_return_args\n",
    "\n",
    "    # build computation graph\n",
    "    agent.build_computation_graph()\n",
    "\n",
    "    # tensorflow: config, session, variable initialization\n",
    "    agent.init_tf_sess()\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Training Loop\n",
    "    #========================================================================================#\n",
    "\n",
    "    total_timesteps = 0\n",
    "    for itr in range(n_iter):\n",
    "        print(\"********** Iteration %i ************\"%itr)\n",
    "        paths, timesteps_this_batch = agent.sample_trajectories(itr, env)\n",
    "        total_timesteps += timesteps_this_batch\n",
    "\n",
    "        # Build arrays for observation, action for the policy gradient update by concatenating \n",
    "        # across paths\n",
    "        ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "        re_n = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        next_ob_no = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminal_n = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "\n",
    "        # Call tensorflow operations to:\n",
    "        # (1) update the critic, by calling agent.update_critic\n",
    "        # (2) use the updated critic to compute the advantage by, calling agent.estimate_advantage\n",
    "        # (3) use the estimated advantage values to update the actor, by calling agent.update_actor\n",
    "        # YOUR CODE HERE\n",
    "        agent.update_critic(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        adv_n = agent.estimate_advantage(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        agent.update_actor(ob_no, ac_na, adv_n)\n",
    "        \n",
    "        # Log diagnostics\n",
    "        returns = [path[\"reward\"].sum() for path in paths]\n",
    "        ep_lengths = [pathlength(path) for path in paths]\n",
    "        print(\"Time\", time.time() - start)\n",
    "        print(\"Iteration\", itr)\n",
    "        print(\"AverageReturn\", np.mean(returns))\n",
    "        print(\"StdReturn\", np.std(returns))\n",
    "        print(\"MaxReturn\", np.max(returns))\n",
    "        print(\"MinReturn\", np.min(returns))\n",
    "        print(\"EpLenMean\", np.mean(ep_lengths))\n",
    "        print(\"EpLenStd\", np.std(ep_lengths))\n",
    "        print(\"TimestepsThisBatch\", timesteps_this_batch)\n",
    "        print(\"TimestepsSoFar\", total_timesteps)\n",
    "        plot(itr,np.mean(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Провдеите эксперименты с несоклькими окружениями и разными параметрами: __CartPole-v0__ (num_target_updates=[1,1,100], num_grad_steps_per_target_update=[1,10,100], max_path_length=3000). Выберете лучшее сочетание для num_target_updates и num_grad_steps_per_target_update и с этми параметрами проверьте __InvertedPendulum-v2__ (gamma=0.9, learning_rate=0.01, min_timesteps_per_batch=5000, size=64, max_path_length=-1.), __HalfCheetah-v2__ (gamma=0.95, learning_rate=0.02, min_timesteps_per_batch=30000, size=32, max_path_length=-1.). Для каждого окружения опишите особенности окружения, постройте графики качества работы агента, прокомментируйте результаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_AC(\n",
    "        exp_name=\"CartPole-v0\",\n",
    "        env_name=\"CartPole-v0\",\n",
    "        n_iter=100,\n",
    "        gamma=1,\n",
    "        min_timesteps_per_batch=1000,\n",
    "        max_path_length=-1.,\n",
    "        learning_rate=5e-3,\n",
    "        num_target_updates=10,\n",
    "        num_grad_steps_per_target_update=10,\n",
    "        animate=True,\n",
    "        normalize_advantages=False,\n",
    "        seed=1,\n",
    "        n_layers=2,\n",
    "        size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
