{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 8: Планирование и RL\n",
    "\n",
    "\n",
    "## Майнор ВШЭ, 13.03.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритмы Dyna-Q и MCTS\n",
    "\n",
    "На данном семинаре мы вначале напишем класс DynaQAgent, котрый будет представлять из себя табличного Dyna-Q агента. Проведем его сравнение с Q-learning агентом, после рассмотрим другой вариант планирования: MCTS. Все варианты рассмотри на примере Taxi-v2.\n",
    "\n",
    "После этого сравним оба варианта планирования друг с другом на примере игры в Шахматы (https://github.com/genyrosk/gym-chess)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q\n",
    "\n",
    "Создадим класс DynaQAgent, который будет решать задачу обучения методом Dyna-Q, согласно приведенному ниже алгоритму.\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/1600/1*jGiNKLkGwmdjzhCUrSNJIg.png width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile dyna_q.py\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class DynaQAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, n_steps, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._memoryModel = []\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        self._qvalues[state][action] = value\n",
    "        \n",
    "    def get_model(self, state, action):\n",
    "        return self._memoryModel[state][action]\n",
    "\n",
    "    def set_model(self, state, action, value):\n",
    "        self._memoryModel[state][action] = value\n",
    "        \n",
    "    def get_value(self, state):\n",
    "        \n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max(self.get_qvalue(state, action) for action in possible_actions)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с алгоритмом представленном выше будем добавлять методы к нашему классу. Начнем с обновления Q-значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state):\n",
    "    alpha = self.alpha\n",
    "    gamma = self.discount\n",
    "    n_steps = self.n_steps\n",
    "    Q_update = self.get_qvalue(state, action) + alpha*(reward + \n",
    "                gamma*self.get_value(next_state) - self.get_qvalue(state, action))\n",
    "    self.set_qvalue(state, action, Q_update)\n",
    "    self._memoryModel.append((state, action, reward, next_state))\n",
    "    self.search()\n",
    "        \n",
    "DynaQAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - этап планирования (поиска)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self):\n",
    "    n_steps = self.n_steps\n",
    "    alpha = self.alpha\n",
    "    gamma = self.discount\n",
    "    for _ in range(n_steps):\n",
    "        state, action, reward, next_state = random.choice(self._memoryModel)\n",
    "\n",
    "        Q_update = self.get_qvalue(state, action) + alpha*(reward + \n",
    "                gamma*self.get_value(next_state) - self.get_qvalue(state, action))\n",
    "        self.set_qvalue(state, action, Q_update)\n",
    "\n",
    "DynaQAgent.search = search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии со сформированной таблицей значений - выбираем действия и используем $\\epsilon$-жадную стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(self, state):\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "    # If there are no legal actions, return None\n",
    "    if len(possible_actions) == 0:\n",
    "        return None\n",
    "\n",
    "    array_qvalues = [self.get_qvalue(state, action) for action in possible_actions]\n",
    "    return possible_actions[array_qvalues.index(max(array_qvalues))]\n",
    "\n",
    "DynaQAgent.get_best_action = get_best_action\n",
    "\n",
    "def get_action(self, state):\n",
    "    # Pick Action\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "    action = None\n",
    "\n",
    "    # If there are no legal actions, return None\n",
    "    if len(possible_actions) == 0:\n",
    "        return None\n",
    "\n",
    "    # agent parameters:\n",
    "    epsilon = self.epsilon\n",
    "\n",
    "    if random.random() > epsilon:\n",
    "        chosen_action = self.get_best_action(state)\n",
    "    else:\n",
    "        chosen_action = random.choice(possible_actions)\n",
    "\n",
    "    return chosen_action\n",
    "\n",
    "DynaQAgent.get_action = get_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с Q-learning\n",
    "\n",
    "Сравним получившийся алгоритм с классическим Q-learning на примере Taxi-v2 enviroment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим среду и агентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "n_actions = env.action_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "a = defaultdict(lambda:0)\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.05, discount=0.9,\n",
    "                          get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_dynaq = DynaQAgent(alpha=0.25, epsilon=0.05, discount=0.9, n_steps=50,\n",
    "                         get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также напишем функцию play_and_train, которая будет полностью прогонять игру, тренировать агента на каждой паре состояние-действие и возвращать полученную награду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYNA-Q mean reward = 6.24\n",
      "QLEARNING mean reward = 4.82\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def moving_average(x, span=100): return DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_dynaq, rewards_ql = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rewards_dynaq.append(play_and_train(env, agent_dynaq))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print('DYNA-Q mean reward =', np.mean(rewards_dynaq[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        plt.plot(moving_average(rewards_dynaq), label='dyna-q')\n",
    "        plt.plot(moving_average(rewards_ql), label='q-learning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как несложно заметить что Dyna-Q, что Q-learning сходятся к одному и тому же значению, однако в случае с Dyna-Q это происходит гораздо быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Монте-Карло поиск по дереву - MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс UCTAgent, который реализует алгоритм UCT, общая схема которого приведена ниже:\n",
    "\n",
    "<img src=\"MCTS.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "from copy import copy\n",
    "from math import sqrt, log\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, possible_actions, is_terminal):\n",
    "        # Состояние\n",
    "        self.state = state\n",
    "        # Является ли состояние конечным\n",
    "        self.is_terminal = is_terminal\n",
    "        # Словарь со значениями функции Q(s, a)\n",
    "        self._qvalues = defaultdict(lambda: 0)\n",
    "        # Словарь с количеством выбора каждого действия в состоянии state\n",
    "        self._actions_used = defaultdict(lambda: 0)\n",
    "        # Количество посещений данного состояния\n",
    "        self.visits = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCTAgent:\n",
    "    def __init__(self, get_legal_actions, rollouts=5, horizon=100, gamma=0.9, ucb_const = 2, is_Chess = False):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self.rollouts = rollouts\n",
    "        self.horizon = horizon\n",
    "        self.ucb = ucb_const\n",
    "        self.Tree = {}\n",
    "        self.state_action = {}\n",
    "        self.chess = {}\n",
    "        self.gamma = gamma\n",
    "        self.is_Chess = is_Chess\n",
    "        \n",
    "    def to_board(self, state):\n",
    "        if self.is_Chess:\n",
    "            for name, value in self.chess.items():\n",
    "                if value['board'] == state['board']:\n",
    "                    return name\n",
    "            self.chess[len(self.chess.keys())] = state\n",
    "            return len(self.chess.keys())\n",
    "        else:\n",
    "            return state\n",
    "    \n",
    "    def get_action(self, env, state):\n",
    "        for _ in range(self.rollouts):\n",
    "            self.simulate(copy(env), copy(state))\n",
    "        return self.ucb_select(self.Tree[self.to_board(state)])\n",
    "        \n",
    "    def simulate(self, env, state):\n",
    "        states, reward, done, env, t = self.sim_tree(env, state)\n",
    "        total_reward = self.sim_default(env, states[-1], reward, done, t)\n",
    "        self.back_up(states, total_reward)\n",
    "        self.state_action = {}\n",
    "        \n",
    "    def sim_tree(self, env, init_state):    \n",
    "        t = 0\n",
    "        done = False\n",
    "        states = []\n",
    "        state = self.to_board(init_state)\n",
    "        total_reward = 0\n",
    "        while not done and (t < self.horizon):\n",
    "            if not state in self.Tree.keys():\n",
    "                if self.is_Chess:\n",
    "                    self.Tree[state] = Node(self.chess[state], self.get_legal_actions(self.chess[state]), done)\n",
    "                else:\n",
    "                    self.Tree[state] = Node(state, self.get_legal_actions(state), done)\n",
    "                states.append(state)\n",
    "                return states, total_reward, done, env, t\n",
    "            states.append(state)\n",
    "            action = self.ucb_select(self.Tree[state])\n",
    "            self.state_action[state] = action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = self.to_board(next_state)\n",
    "            t += 1\n",
    "        return states, total_reward, done, env, t\n",
    "            \n",
    "    def sim_default(self, env, state, prev_reward, done, t):\n",
    "        total_reward = prev_reward\n",
    "        while not done:# and (t < self.horizon):\n",
    "            action = self.default_policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.state_action[state] = action\n",
    "            state = self.to_board(next_state)\n",
    "            total_reward += reward\n",
    "            t += 1\n",
    "        return total_reward\n",
    "        \n",
    "    def default_policy(self, state):\n",
    "        if self.is_Chess:\n",
    "            return random.choice(self.get_legal_actions(self.chess[state]))\n",
    "        else:\n",
    "            return random.choice(self.get_legal_actions(state))\n",
    "\n",
    "        \n",
    "    def back_up(self, states, total_reward):\n",
    "        state_action = self.state_action\n",
    "        for state in states:\n",
    "            self.Tree[state].visits += 1\n",
    "            action = state_action[state]\n",
    "            self.Tree[state]._actions_used[action] += 1\n",
    "            self.Tree[state]._qvalues[action] += self.gamma*(total_reward - self.Tree[state]._qvalues[action]\n",
    "                                                 ) / float(self.Tree[state]._actions_used[action])\n",
    "        \n",
    "    def ucb_select(self, state_node):\n",
    "        c = self.ucb\n",
    "        state = state_node.state\n",
    "        if self.is_Chess:\n",
    "            possible_actions = self.get_legal_actions(self.chess[state])\n",
    "        else:\n",
    "            possible_actions = self.get_legal_actions(state)\n",
    "        array_qvalues = [state_node._qvalues[action] + c*sqrt(log(state_node.visits)/state_node._actions_used[action]) \n",
    "                         if action in state_node._actions_used.keys() else float('+inf') for action in possible_actions ]\n",
    "        #print(possible_actions)\n",
    "        return possible_actions[array_qvalues.index(max(array_qvalues))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также сравним UCT с предыдущими алгоритмами на все том же Taxi-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\", )\n",
    "n_actions = env.action_space.n\n",
    "env.render()\n",
    "agent_uct = UCTAgent(get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "from qlearning import QLearningAgent\n",
    "# from dyna_q import DynaQAgent\n",
    "\n",
    "\n",
    "a = defaultdict(lambda:0)\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.05, discount=0.9,\n",
    "                          get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_dynaq = DynaQAgent(alpha=0.25, epsilon=0.05, discount=0.9, n_steps=50,\n",
    "                         get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train_ts(env, agent, t_max=10**4):   \n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(copy(env), s)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def moving_average(x, span=100): return DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_dynaq, rewards_ql, rewards_uct = [], [], []\n",
    "\n",
    "for i in range(51):\n",
    "    rewards_dynaq.append(play_and_train(env, agent_dynaq))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "    rewards_uct.append(play_and_train_ts(env, agent_uct))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print('DYNA-Q mean reward =', np.mean(rewards_dynaq[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        print('UCT mean reward =', np.mean(rewards_uct[-100:]))\n",
    "        plt.plot(moving_average(rewards_uct), label='uct')\n",
    "        plt.plot(moving_average(rewards_dynaq), label='dyna-q')\n",
    "        plt.plot(moving_average(rewards_ql), label='q-learning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что благодаря планированию до конца, UCT практически сразу находит оптимальную стратегию. Также видно, что среднее значение награды для UCT выше, чем для Dyna-Q и Q-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
