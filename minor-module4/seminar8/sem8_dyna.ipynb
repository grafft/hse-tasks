{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 8: Планирование и RL\n",
    "\n",
    "\n",
    "## Майнор ВШЭ, 13.03.2019\n",
    "\n",
    "Опрос - https://goo.gl/forms/siceP1Q7QUQaCiMD2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритмы Dyna-Q и MCTS\n",
    "\n",
    "На данном семинаре мы вначале напишем класс DynaQAgent, котрый будет представлять из себя табличного Dyna-Q агента. Проведем его сравнение с Q-learning агентом, после рассмотрим другой вариант планирования: MCTS. Все варианты рассмотри на примере Taxi-v2.\n",
    "\n",
    "После этого сравним оба варианта планирования друг с другом на примере игры в Шахматы (https://github.com/genyrosk/gym-chess)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q\n",
    "\n",
    "Создадим класс DynaQAgent, который будет решать задачу обучения методом Dyna-Q, согласно приведенному ниже алгоритму.\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/1600/1*jGiNKLkGwmdjzhCUrSNJIg.png width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile dyna_q.py\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class DynaQAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, n_steps, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._memoryModel = []\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        self._qvalues[state][action] = value\n",
    "        \n",
    "    def get_model(self, state, action):\n",
    "        return self._memoryModel[state][action]\n",
    "\n",
    "    def set_model(self, state, action, value):\n",
    "        self._memoryModel[state][action] = value\n",
    "        \n",
    "    def get_value(self, state):\n",
    "        \n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max(self.get_qvalue(state, action) for action in possible_actions)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с алгоритмом представленном выше будем добавлять методы к нашему классу. Начнем с обновления Q-значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state):\n",
    "    alpha = self.alpha\n",
    "    gamma = self.discount\n",
    "    n_steps = self.n_steps\n",
    "    ### Ваш код здесь\n",
    "    #Q_update = \n",
    "    self.set_qvalue(state, action, Q_update)\n",
    "    self._memoryModel.append((state, action, reward, next_state))\n",
    "    self.search()\n",
    "        \n",
    "DynaQAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - этап планирования (поиска)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self):\n",
    "    n_steps = self.n_steps\n",
    "    alpha = self.alpha\n",
    "    gamma = self.discount\n",
    "    for _ in range(n_steps):\n",
    "        ### Ваш код здесь\n",
    "        # выбираем случайно из памяти\n",
    "        #state, action, reward, next_state = \n",
    "        \n",
    "        ### Ваш код здесь\n",
    "        # Q_update = \n",
    "        self.set_qvalue(state, action, Q_update)\n",
    "\n",
    "DynaQAgent.search = search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии со сформированной таблицей значений - выбираем действия и используем $\\epsilon$-жадную стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(self, state):\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "    # If there are no legal actions, return None\n",
    "    if len(possible_actions) == 0:\n",
    "        return None\n",
    "\n",
    "    ### Ваш код здесь\n",
    "    # лучшее по полезности действие\n",
    "    # action = \n",
    "    return action\n",
    "\n",
    "DynaQAgent.get_best_action = get_best_action\n",
    "\n",
    "def get_action(self, state):\n",
    "    # Pick Action\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "    action = None\n",
    "\n",
    "    # If there are no legal actions, return None\n",
    "    if len(possible_actions) == 0:\n",
    "        return None\n",
    "\n",
    "    # agent parameters:\n",
    "    ### Ваш код здесь\n",
    "    # e-жадная стратегия\n",
    "    # chosen_action = \n",
    "\n",
    "    return chosen_action\n",
    "\n",
    "DynaQAgent.get_action = get_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с Q-learning\n",
    "\n",
    "Сравним получившийся алгоритм с классическим Q-learning на примере Taxi-v2 enviroment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим среду и агентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "n_actions = env.action_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "a = defaultdict(lambda:0)\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.05, discount=0.9,\n",
    "                          get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_dynaq = DynaQAgent(alpha=0.25, epsilon=0.05, discount=0.9, n_steps=50,\n",
    "                         get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также напишем функцию play_and_train, которая будет полностью прогонять игру, тренировать агента на каждой паре состояние-действие и возвращать полученную награду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def moving_average(x, span=100): return DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_dynaq, rewards_ql = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rewards_dynaq.append(play_and_train(env, agent_dynaq))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print('DYNA-Q mean reward =', np.mean(rewards_dynaq[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        plt.plot(moving_average(rewards_dynaq), label='dyna-q')\n",
    "        plt.plot(moving_average(rewards_ql), label='q-learning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как несложно заметить что Dyna-Q, что Q-learning сходятся к одному и тому же значению, однако в случае с Dyna-Q это происходит гораздо быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Монте-Карло поиск по дереву - MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс UCTAgent, который реализует алгоритм UCT, общая схема которого приведена ниже:\n",
    "\n",
    "<img src=\"MCTS.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "from copy import copy\n",
    "from math import sqrt, log\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, possible_actions, is_terminal):\n",
    "        # Состояние\n",
    "        self.state = state\n",
    "        # Является ли состояние конечным\n",
    "        self.is_terminal = is_terminal\n",
    "        # Словарь со значениями функции Q(s, a)\n",
    "        self._qvalues = defaultdict(lambda: 0)\n",
    "        # Словарь с количеством выбора каждого действия в состоянии state\n",
    "        self._actions_used = defaultdict(lambda: 0)\n",
    "        # Количество посещений данного состояния\n",
    "        self.visits = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCTAgent:\n",
    "    def __init__(self, get_legal_actions, rollouts=5, horizon=100, gamma=0.9, ucb_const = 2, is_Chess = False):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self.rollouts = rollouts\n",
    "        self.horizon = horizon\n",
    "        self.ucb = ucb_const\n",
    "        self.Tree = {}\n",
    "        self.state_action = {}\n",
    "        self.chess = {}\n",
    "        self.gamma = gamma\n",
    "        self.is_Chess = is_Chess\n",
    "        \n",
    "    def to_board(self, state):\n",
    "        if self.is_Chess:\n",
    "            for name, value in self.chess.items():\n",
    "                if value['board'] == state['board']:\n",
    "                    return name\n",
    "            self.chess[len(self.chess.keys())] = state\n",
    "            return len(self.chess.keys())\n",
    "        else:\n",
    "            return state\n",
    "    \n",
    "    def get_action(self, env, state):\n",
    "        for _ in range(self.rollouts):\n",
    "            self.simulate(copy(env), copy(state))\n",
    "        return self.ucb_select(self.Tree[self.to_board(state)])\n",
    "        \n",
    "    def simulate(self, env, state):\n",
    "        states, reward, done, env, t = self.sim_tree(env, state)\n",
    "        total_reward = self.sim_default(env, states[-1], reward, done, t)\n",
    "        self.back_up(states, total_reward)\n",
    "        self.state_action = {}\n",
    "        \n",
    "    def sim_tree(self, env, init_state):    \n",
    "        t = 0\n",
    "        done = False\n",
    "        states = []\n",
    "        state = self.to_board(init_state)\n",
    "        total_reward = 0\n",
    "        while not done and (t < self.horizon):\n",
    "            if not state in self.Tree.keys():\n",
    "                if self.is_Chess:\n",
    "                    self.Tree[state] = Node(self.chess[state], self.get_legal_actions(self.chess[state]), done)\n",
    "                else:\n",
    "                    self.Tree[state] = Node(state, self.get_legal_actions(state), done)\n",
    "                states.append(state)\n",
    "                return states, total_reward, done, env, t\n",
    "            states.append(state)\n",
    "            action = self.ucb_select(self.Tree[state])\n",
    "            self.state_action[state] = action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = self.to_board(next_state)\n",
    "            t += 1\n",
    "        return states, total_reward, done, env, t\n",
    "            \n",
    "    def sim_default(self, env, state, prev_reward, done, t):\n",
    "        total_reward = prev_reward\n",
    "        while not done:# and (t < self.horizon):\n",
    "            action = self.default_policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.state_action[state] = action\n",
    "            state = self.to_board(next_state)\n",
    "            total_reward += reward\n",
    "            t += 1\n",
    "        return total_reward\n",
    "        \n",
    "    def default_policy(self, state):\n",
    "        if self.is_Chess:\n",
    "            return random.choice(self.get_legal_actions(self.chess[state]))\n",
    "        else:\n",
    "            return random.choice(self.get_legal_actions(state))\n",
    "\n",
    "        \n",
    "    def back_up(self, states, total_reward):\n",
    "        state_action = self.state_action\n",
    "        for state in states:\n",
    "            self.Tree[state].visits += 1\n",
    "            action = state_action[state]\n",
    "            self.Tree[state]._actions_used[action] += 1\n",
    "            self.Tree[state]._qvalues[action] += self.gamma*(total_reward - self.Tree[state]._qvalues[action]\n",
    "                                                 ) / float(self.Tree[state]._actions_used[action])\n",
    "        \n",
    "    def ucb_select(self, state_node):\n",
    "        c = self.ucb\n",
    "        state = state_node.state\n",
    "        if self.is_Chess:\n",
    "            possible_actions = self.get_legal_actions(self.chess[state])\n",
    "        else:\n",
    "            possible_actions = self.get_legal_actions(state)\n",
    "        array_qvalues = [state_node._qvalues[action] + c*sqrt(log(state_node.visits)/state_node._actions_used[action]) \n",
    "                         if action in state_node._actions_used.keys() else float('+inf') for action in possible_actions ]\n",
    "        #print(possible_actions)\n",
    "        return possible_actions[array_qvalues.index(max(array_qvalues))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также сравним UCT с предыдущими алгоритмами на все том же Taxi-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\", )\n",
    "n_actions = env.action_space.n\n",
    "env.render()\n",
    "agent_uct = UCTAgent(get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "from qlearning import QLearningAgent\n",
    "# from dyna_q import DynaQAgent\n",
    "\n",
    "\n",
    "a = defaultdict(lambda:0)\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.05, discount=0.9,\n",
    "                          get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_dynaq = DynaQAgent(alpha=0.25, epsilon=0.05, discount=0.9, n_steps=50,\n",
    "                         get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train_ts(env, agent, t_max=10**4):   \n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(copy(env), s)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def moving_average(x, span=100): return DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_dynaq, rewards_ql, rewards_uct = [], [], []\n",
    "\n",
    "for i in range(51):\n",
    "    rewards_dynaq.append(play_and_train(env, agent_dynaq))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "    rewards_uct.append(play_and_train_ts(env, agent_uct))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print('DYNA-Q mean reward =', np.mean(rewards_dynaq[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        print('UCT mean reward =', np.mean(rewards_uct[-100:]))\n",
    "        plt.plot(moving_average(rewards_uct), label='uct')\n",
    "        plt.plot(moving_average(rewards_dynaq), label='dyna-q')\n",
    "        plt.plot(moving_average(rewards_ql), label='q-learning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что благодаря планированию до конца, UCT практически сразу находит оптимальную стратегию. Также видно, что среднее значение награды для UCT выше, чем для Dyna-Q и Q-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
