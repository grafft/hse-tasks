{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 6: Policy gradient\n",
    "\n",
    "\n",
    "## Майнор ВШЭ, 28.02.2019\n",
    "\n",
    "Опрос на сегодня - https://goo.gl/forms/RrNM85N4xLTkb7yg1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых задачах для нахождения удовлетворяющей стратегии необязательно изучать структуру всей среды. Например, в задаче поднятия кубика робототехнической рукой вместо точной аппроксимации $Q(s,a)$ достаточно знать, что выгоднее двигаться вправо, если кубик справа, и влево в ином случае. Алгоритм Reinforce (Monte Carlo policy gradient) - это алгоритм поиска стратегий, в котором параметры, задающие стохастическую стратегию, изменяются в соответствии с градиентом математического ожидания награды: \n",
    "\n",
    "$J(\\theta)=\\mathbb E_{\\tau\\sim p_{\\theta}(\\tau)}(\\sum_t\\gamma^tr(s_t,a_t))$\n",
    "\n",
    "$\\theta\\leftarrow\\theta+\\alpha\\nabla_{\\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = #Создайте среду Acrobot-v1\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "gamma = 0.95\n",
    "\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Action Space\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задайте переменные, которые будут подаваться на вход нейронной сети\n",
    "#Состояния\n",
    "observations =\n",
    "#Совершенные действия\n",
    "actions =\n",
    "# \"G = r + gamma*r' + gamma^2*r'' + ...\"\n",
    "discounted_episode_rewards =\n",
    "all_inputs = [observations, actions, discounted_episode_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Определяем архитектуру сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия задается весами нейронной сети и является стохастической, т.е. в состоянии $s$ она представляет себой некоторое распределение $\\pi_\\theta(s)$, поэтому на вход сети будет родаваться состояние $s$, а на выходе будут вероятности действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "#Задайте внутренние и выходной слои нейронной сети. \n",
    "nn1 = \n",
    "nn2 = \n",
    "#Выход последнего слоя преобразуется в стохастическую стратегию, поэтому количество нейронов должно быть соответствующим\n",
    "nn3 =  \n",
    "\n",
    "probs_out = tf.nn.softmax(nn3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Функция потерь\n",
    "\n",
    "Теперь определите функцию потерь (Crossentropy loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент стратегии выглядит следующим образом $\\nabla_{\\theta}J_{\\theta}\\approx\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{i,t}|s_{i,t})R $. Чтобы автоматически вычислить градиент, необходимо задать граф, который имеет градиент такого же вида. Для этого будем использовать \"псевдо\" функцию потерь: $\\tilde{J}_{\\theta}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T\\log\\pi_{\\theta}(a_{i,t}|s_{i,t})R $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_prob = #примените кросс энтропию к последнему слою сети tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "loss = #умножьте на дисконтированную награду и возьмите среднее, чтобы получить искомую функцию потерь\n",
    "    \n",
    "optimizer = #задайте метод оптимизации, который хотите использовать\n",
    "train_op = optimizer.minimize(loss, global_step=tf.contrib.framework.get_global_step())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент имеет большую дисперсию. Для ее уменьшения можно использовать следующую функцию потерь: $\\tilde{J}_{\\theta}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^Tlog\\pi_{\\theta}(a_{i,t}|s_{i,t})(R-b) $.\n",
    "\n",
    "$b$ - может быть константой (это не сильно улучшит работу алгоритма), а может быть функцией от $s$, например $V(s)$. $V(s)$ может быть аппроксимированна другой нейронной сетью. Настройка может проходить по методу наименьших квадратов: необходимо задать функцию ошибок и можно добавить ее к loss'у с некоторым коэффициентом или минимизировать отдельно. Пример: https://github.com/yrlu/reinforcement_learning/blob/master/policy_gradient/reinforce_w_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "max_episodes = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            probs = #Получите распределение вероятностей действий в соответствии со стохастической стратегией агента\n",
    "            \n",
    "            action = #Выберете действие (по распределению) \n",
    "\n",
    "            new_state, reward, done, info = \n",
    "\n",
    "            episode_states.append(state)\n",
    "            action_ = np.zeros(n_actions)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                \n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                discounted_rewards = #Посчитайте дисконтированную награду\n",
    "                                \n",
    "                #Feedforward, gradient and backpropagation\n",
    "                loss_, _ = #Посчитайте значение функции потерь и настройте веса сети при помощи оптимизатора\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Проводим эксперименты.\n",
    "\n",
    "Постройте график получаемого суммарного вознаграждения от номера эпизода.\n",
    "\n",
    "Сделайте тоже самое для задачи CartPole-v0. \n",
    "\n",
    "Усложните архитектуру сети. Улучшает ли это производительность?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные материалы\n",
    "\n",
    "Пример алгоритма DDPG (https://arxiv.org/pdf/1509.02971.pdf) - алгоритм использующий градиент стратегий и позволяющий работать с непрерывным множеством действий:\n",
    "\n",
    "https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
