{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторная модель текста в NLTK\n",
    "### Задание 1\n",
    "Возьмем файл alice.txt с прошлого семинара. Мы уже научились разбивать его на токены и удалять стоп-слова.\n",
    "Используя nltk.FreqDist, выведите 20 самых часто встречающихся слов в тексте (не удаляя стоп символы).\n",
    "\n",
    "Ответьте на вопросы:\n",
    "\n",
    "    Какое слово стоит на 1 месте?\n",
    "    Сколько раз встречается слово, стоящее на 20 месте?\n",
    "    Сколько не стоп-слов среди первых десяти?\n",
    "    Сколько раз встречается слово \"крокет\"? (понятно, что оно не входит в top20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d1 = nltk.FreqDist(l1)\n",
    "# d1.most_common(10)\n",
    "\n",
    "# d3['...'], d3.freq('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Избавьтесь от стоп-слов с помощью списка stopwords из nltk.corpus. Используйте pymorphy2 или Mystem, чтобы привести слова к нормальной форме. Ответьте на вопросы:\n",
    "\n",
    "    Какое слово стоит на 1 месте?\n",
    "    Сколько раз встречается слово, стоящее на 20 месте?\n",
    "    Остались ли какие-то стоп-слова среди первых 10? 20?\n",
    "    Сколько раз встречается слово \"крокет\"? Почему результат изменился?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# morph = pymorphy2.MorphAnalyzer()\n",
    "# l3 = [morph.parse(token)[0].normal_form for token in l1 if not token in stopwords.words('russian')]\n",
    "\n",
    "# d3['...'], d3.freq('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте пострим кумулятивный график частот слов. Как он выглядит и почему так? Попробуйте поменять количество слов, какие тенденции вы видите?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3.plot (50, cumulative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Найдите распределение длин слов в тексте, выведите top5 и ответьте на вопросы:\n",
    "\n",
    "    Сколько всего в данном тексте вариантов длин слов (вместе со стоп-словами)?\n",
    "    Слов какой длины в тексте больше всего?\n",
    "    Какую долю занимают слова этой длины?\n",
    "    Сколько слов длины 1?\n",
    "\n",
    "Выведите в алфавитном поредке длинные слова (длиннее 15 символов).\n",
    "\n",
    "    Сколько их?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fdist = nltk.FreqDist(len(w) for w in l1)  \n",
    "# print(fdist)  \n",
    "# print (fdist.most_common(5))\n",
    "# print (fdist.freq(fdist.max()))\n",
    "\n",
    "# long_words = [w for w in l1 if len(w) > 10]\n",
    "# sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "\n",
    "Превратите строку в NLTK-текст и проверьте, как работают спецфункции, на собственных примерах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text = nltk.Text(l1)\n",
    "# print (text.concordance(\".....\")) \n",
    "# print (text.similar('......'))\n",
    "# print (text.common_contexts([\".....\", \"......\"]))\n",
    "# print (text.collocations(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эмбеддинги\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1228)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем куски датасета твитов ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/ilkte35m35l38mr/negative.sql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем лемматизированные статьи без стоп-слов и создаем массив текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(\"negative.csv?dl=0\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"positive.csv?dl=0\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df_pos['text'] = df_pos[3]\n",
    "df_neg['text'] = df_neg[3]\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df = df[['text', 'sent']]\n",
    "%time df.text = df.text.apply(words_only)\n",
    "%time df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [df.text.iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(texts, size=100, window=5, min_count=5, workers=4)\n",
    "model.save(\"word2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# здесь можно посмотреть все слова, к которым мы имеем вектора\n",
    "# model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем обученную модель (для скорости) - https://yadi.sk/d/dhwCKThcStprig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = Word2Vec.load(\"sent_w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно воспользjваться функциями поиска похожих / непохожих слов на данное\n",
    "\n",
    "    model.most_similar\n",
    "    model.doesnt_match\n",
    "    etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"корпоратив\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"король\",\"добрый\"], negative=[\"хороший\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"борщ сметана макароны пирожок котлета\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Выделите top 100 самых частотных слов и получите для них только что обученные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fd = FreqDist()\n",
    "# your code is hear\n",
    "\n",
    "top_words = # your code is hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model['рабочий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words_vec = # your code is hear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем вектора. Для этого воспользуемся методом снижения размерности t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "top_words_tsne = tsne.fit_transform(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"word2vec T-SNE for most common words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=top_words_tsne[:,0],\n",
    "                                    x2=top_words_tsne[:,1],\n",
    "                                    names=top_words))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нарисуем облако слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in fd.most_common(100)]\n",
    "wd = WordCloud(background_color = 'white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import  ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100)) \n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=top_words);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('w2v_cluster.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели биграм в NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [name.strip().lower() for name in open('dinos.txt').readlines()]\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = [char  for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "\n",
    "print(list(freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
    "print(cfreq['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l\n",
    "print('p(a) = %1.4f' %unigram_prob('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# можно порождать случайные символы с учётом предыдущих \n",
    "cprob['a'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Напишите функцию для генерации нового имени динозавра фиксированной длины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентные нейронные языковые модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        #teacher forcing\n",
    "        x_str = line\n",
    "        y_str = line[1:] + '\\n'\n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trn_ds.lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trn_ds.ch_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = trn_ds[1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        combined = torch.cat([h_prev, x], dim = 1) # конкатенируем вектора состояния и входа\n",
    "        h = torch.tanh(self.dropout(self.i2h(combined)))\n",
    "        y = self.i2o(combined)\n",
    "        return h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end='') for x in sample_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    newline_idx = trn_ds.ch_to_idx['\\n']\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = random.randint(1, trn_ds.vocab_size-1)\n",
    "        indices = [start_char_idx]\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        \n",
    "        while predicted_char_idx != newline_idx and word_size != 50:\n",
    "            h_prev, y_pred = model(h_prev, x)\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            predicted_char_idx = idx\n",
    "            \n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            h_prev, y_pred = model(h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(model, loss_fn, optimizer, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измените код выше так, чтобы генерировались панграмы – имена динозавров, не содержащие повторяющихся букв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование LSTM нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.linear_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_u = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_c = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, c_prev, h_prev, x):\n",
    "        combined = torch.cat([x, h_prev], 1)\n",
    "        f = torch.sigmoid(self.linear_f(combined))\n",
    "        u = torch.sigmoid(self.linear_u(combined))\n",
    "        c_tilde = torch.tanh(self.linear_c(combined))\n",
    "        c = f*c_prev + u*c_tilde\n",
    "        o = torch.sigmoid(self.linear_o(combined))\n",
    "        h = o*torch.tanh(c)\n",
    "        y = self.i2o(h)\n",
    "        \n",
    "        return c, h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTM(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        c_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        h_prev = torch.zeros_like(c_prev)\n",
    "        idx = random.randint(1, 26)\n",
    "        x = c_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        x[0, idx] = 1\n",
    "        sampled_indexes = [idx]\n",
    "        n_chars = 1\n",
    "        newline_char_idx = trn_ds.ch_to_idx['\\n']\n",
    "        while n_chars != 50 and idx != newline_char_idx:\n",
    "            c_prev, h_prev, y_pred = model(c_prev, h_prev, x)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=torch.softmax(y_pred, 1).cpu().numpy().ravel())\n",
    "            sampled_indexes.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            \n",
    "            n_chars += 1\n",
    "            \n",
    "            if n_chars == 50:\n",
    "                sampled_indexes.append(newline_char_idx)\n",
    "                \n",
    "    model.train()\n",
    "    return sampled_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        c_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        h_prev = torch.zeros_like(c_prev)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            c_prev, h_prev, y_pred = model(c_prev, h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(model, loss_fn, optimizer, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать функцию get_prob(), оценивающую веростность порождения одной строки (из файла) и найти самую вероятную строку, порождаемую каждой из трех языковых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя функцию get_prob(), написать функцию perplexety, оценивающую перплексию каждой из трех языковых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить в функцию сэмплирования возможность использовать т.н. температуру. Сейчас сэмплирование следующего символа осуществляется с использованием np.random.choice, где вероятность каждого символа получена на выходе из softmax:\n",
    "\n",
    "p=torch.softmax(y_pred, 1).cpu().numpy().ravel().\n",
    "\n",
    "Температура сэмплирования определяется параметром $T$ и преобразует вероятности следующим образом: $p_i = \\frac{p_i^{1/T}}{\\sum_j p_j^{1/T}}$ .\n",
    "\n",
    "Проведите эксперименты с разными значениями $T \\in [0.5, 1, 2]$. Как разные значения $T$ влияют на генерируемые строки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте beam search для генерации строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
