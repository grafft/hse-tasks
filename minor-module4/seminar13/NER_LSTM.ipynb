{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение именованных сущеностей\n",
    "\n",
    "Опрос на сегодня - https://forms.gle/HVfV32cRHCDbNrbK9.\n",
    "\n",
    "На этом семинаре будем заниматься выделением именованных сущеностей из текстов\n",
    "\n",
    "Загрузите эмбеддинги c https://nlp.stanford.edu/projects/glove/ или другие, которые вам нравятся и пропишите путь к ним. Можно скачать один файл - https://yadi.sk/i/SFgsTWW6RoP90w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Идея для улучшения (быстродействия и потребления памяти) - сначала считать корпус, а потом эмбеддинги, и запоминать \n",
    "#эмбеддинги только тех слов, которые есть в корпусе\n",
    "word_embeddings_path = 'glove.6B.50d.txt'\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Загружаем эмбеддинги\n",
    "with open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        # Совсем короткие строки пропускаем\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        # Встретив первую подходящую строку, фиксируем размер эмбеддингов\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Также нициализируем эмбеддинги для паддингов и неизвестных слов\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        # После этого все эмбеддинги должны быть одинаковой длины\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем впспомогательную функцию, которая определяет тип токена (число и капитализация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "#Идея для улучшения - перейдите к шаблонам капитализации (как в Nadeau and Sekine 2007)\n",
    "case2idx = {'numeric': 0, 'all_lower':1, 'all_upper':2, 'initial_upper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "case_embeddings = np.identity(len(case2idx), dtype='float32')\n",
    "\n",
    "def get_casing(word, case_lookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    num_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_digits += 1\n",
    "            \n",
    "    digit_fraction = num_digits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Цифра\n",
    "        casing = 'numeric'\n",
    "    elif digit_fraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #Все буквы маленькие\n",
    "        casing = 'all_lower'\n",
    "    elif word.isupper(): #Все буквы большие\n",
    "        casing = 'all_upper'\n",
    "    elif word[0].isupper(): #Первая большая, остальные маленькие\n",
    "        casing = 'initial_upper'\n",
    "    elif num_digits > 0:\n",
    "        casing = 'contains_digit'  \n",
    "   \n",
    "    return case_lookup[casing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к обработке собственно нашего обучающего и тестового массива данных в CoNLL формате."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COLUMNS = 2\n",
    "WORD_COL_NUM = 0\n",
    "LABEL_COL_NUM = 1\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Получает на вход путь к данным в CoNLL-формате. Выдает массив предложений, разбитых на слова\n",
    "    :param file_path: путь к корпусу в CoNLL-формате\n",
    "    :return: corpus_sentences - массив предложений, разбитых на слова\n",
    "    \"\"\"\n",
    "    corpus_sentences = []\n",
    "    input_sentence = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "\n",
    "            if len(line) == 0 or line[0] == '#':\n",
    "                if len(input_sentence) > 0:\n",
    "                    corpus_sentences.append(input_sentence)\n",
    "                    input_sentence = []\n",
    "                continue\n",
    "            if len(line.split('\\t')) < MAX_COLUMNS:\n",
    "                print(line)\n",
    "                continue\n",
    "            input_sentence.append(line.split('\\t'))\n",
    "\n",
    "    if len(input_sentence) > 0:\n",
    "        corpus_sentences.append(input_sentence)\n",
    "\n",
    "    print(file_path, len(corpus_sentences), \"sentences\")\n",
    "    return corpus_sentences\n",
    "\n",
    "#Пропишите путь к частям корпуса CoNLL-2003\n",
    "train_path = 'conll.train'\n",
    "train_sentences = read_file(train_path)\n",
    "\n",
    "dev_path = 'conll.dev'\n",
    "dev_sentences = read_file(dev_path)\n",
    "\n",
    "test_path = 'conll.test'\n",
    "test_sentences = read_file(test_path)\n",
    "\n",
    "# Часто у нас есть один корпус, из которого мы должны выделить dev и test часть (по 0.1-0.2 выборки). \n",
    "# Напишите код, обрабатывающий такой случай"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем все метки классов и добавляем метку для паддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "label_set.add('PADDING_LABEL')\n",
    "for dataset in [train_sentences, dev_sentences, test_sentences]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            label = token[LABEL_COL_NUM]\n",
    "            label_set.add(label)    \n",
    "\n",
    "# Переводим метки в индексы\n",
    "label2idx = {}\n",
    "idx2label = {}\n",
    "for label in label_set:\n",
    "    label2idx[label] = len(label2idx)\n",
    "    \n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добиваемся того, чтобы предложение всегда начиналось с 1 паддинга и заканчивалось 1 паддингом. Однако, при работе с GPU батчи должны иметь одинаковые размеры. Поэтому имеет смысл разбить предложения на группы так, чтобы все предожения в группе имели одинаковую длину (и значиит предложения одной группы могли попадать в один батч). Простой способ это сделать - добивать все предложения паддингами до одного размера (т. е. иметь одну группу). Большинство предложений короткие, но существуют и очень длинные, поэтому такой способ избыточен.Разумнее иметь более тонкое разбиение на группы - например паддить предложение до следующего 2^n + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices(sentences, word2idx, label2idx, case2idx):   \n",
    "    \n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "    padding_casing = case2idx['PADDING_TOKEN']\n",
    "    padding_idx = word2idx['PADDING_TOKEN'] \n",
    "    padding_label = label2idx['PADDING_LABEL']  \n",
    "    \n",
    "    dataset = []\n",
    "    total_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Индекс первого не паддинга в предложении с паддингами\n",
    "        proper_sentence_start = 1\n",
    "\n",
    "        word_indices = np.array([padding_idx] * (len(sentence) + 2))\n",
    "        case_indices = np.array([padding_casing] * (len(sentence) + 2))\n",
    "        label_indices = np.array([padding_label] * (len(sentence) + 2))\n",
    "        \n",
    "        \n",
    "\n",
    "        for pos_in_sentence, word in enumerate(sentence):\n",
    "\n",
    "            token_unknown, word_idx, case_idx = get_token_indices(word, word2idx, case2idx, unknown_idx)\n",
    "\n",
    "            pos_in_padded_sentence = pos_in_sentence + proper_sentence_start\n",
    "            word_indices[pos_in_padded_sentence] = word_idx\n",
    "            case_indices[pos_in_padded_sentence] = case_idx\n",
    "            label_indices[pos_in_padded_sentence] = label2idx[word[LABEL_COL_NUM]]\n",
    "\n",
    "            # Хотим вычислить процент словоформ, не покрываемых эмбеддингами\n",
    "            total_tokens += 1\n",
    "            if token_unknown:\n",
    "                unknown_tokens += 1\n",
    "\n",
    "        # Все данные для одного предложения помещаем в один массив\n",
    "        dataset.append([word_indices, case_indices, label_indices])\n",
    "        \n",
    "    percent = 0.0\n",
    "    if total_tokens != 0:\n",
    "        percent = float(unknown_tokens) / total_tokens * 100\n",
    "    print(\"{} tokens, {} unknown, {:.3}%\".format(total_tokens, unknown_tokens, percent ))\n",
    "    return dataset\n",
    "\n",
    "def get_token_indices(token, word2idx, case2idx, unknown_idx):\n",
    "\n",
    "    token_unknown = False\n",
    "    # Элемент предложения - несколько колонок, словоформа, при этом, в первой колонке\n",
    "    word = token[WORD_COL_NUM]\n",
    "    # Ищем слово в словаре эмбеддингов, если не нашли, то ищем слово со снятой капитализацией,\n",
    "    # если нет - считаем слово неизвестным\n",
    "    if word2idx.get(word) is not None:\n",
    "        word_idx = word2idx[word]\n",
    "    elif word2idx.get(word.lower()) is not None:\n",
    "        word_idx = word2idx[word.lower()]\n",
    "    else:\n",
    "        word_idx = unknown_idx\n",
    "        token_unknown = True\n",
    "\n",
    "    case_idx = get_casing(word, case2idx)\n",
    "    return token_unknown, word_idx, case_idx\n",
    "\n",
    "train_data = create_matrices(train_sentences, word2idx, label2idx, case2idx)\n",
    "dev_data = create_matrices(dev_sentences, word2idx, label2idx, case2idx)\n",
    "test_data = create_matrices(test_sentences, word2idx, label2idx, case2idx)\n",
    "\n",
    "for sentence in train_datas[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Идея для улучшения - использовать символьные признаки - брать первые  или последние (а лучше и то, и другое) k сиволов\n",
    "#каждого токена и пропускать через рекуррентный или сверточный слой. \n",
    "#Потом получившееся конкатенировать с остальными признакаи в merged_embeddings\n",
    "# Про эту идею можно почитать здесь https://arxiv.org/pdf/1603.01360v1.pdf \n",
    "\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "# На боевом применении должно быть 100 или больше\n",
    "SENTENCE_LSTM_DIM = 10\n",
    "\n",
    "n_out = len(label2idx)\n",
    "\n",
    "tokens_input = Input(dtype='int32', shape=(None,), name='tokens_input')\n",
    "tokens_embedding_layer = Embedding(input_dim=word_embeddings.shape[0], \n",
    "                                   output_dim=word_embeddings.shape[1],\n",
    "                                   weights=[word_embeddings], trainable=False, \n",
    "                                   name='tokens_embeddings')\n",
    "tokens = tokens_embedding_layer(tokens_input)\n",
    "\n",
    "\n",
    "casing_input = Input(dtype='int32', shape=(None,), name='casing_input')\n",
    "casing_embedding_layer = Embedding(input_dim=case_embeddings.shape[0], \n",
    "                                   output_dim=case_embeddings.shape[1],\n",
    "                                   weights=[case_embeddings], trainable=True, \n",
    "                                   name='casing_embeddings')\n",
    "casing = casing_embedding_layer(casing_input)\n",
    "\n",
    "merged_embeddings = concatenate([tokens, casing], name='merged_embeddings')\n",
    "for_lstm = Dropout(0.2)(merged_embeddings)\n",
    "# Если настроите работу на GPU, лучше использовать implementation=2\n",
    "blstm = Bidirectional(LSTM(SENTENCE_LSTM_DIM, return_sequences=True, implementation=0), \n",
    "                      name='blstm')(for_lstm)\n",
    "#Здесь имеет смысл попробовать другие варианиты - GRU, свертки, а также несколько подряд \n",
    "#идущих слоев LSTM или GRU\n",
    "#result = Conv1d(n_out,1, activation='softmax', name='result'))(blstm\n",
    "result = TimeDistributed(Dense(n_out,activation='softmax', name='result'))(blstm)\n",
    "\n",
    "model = Model(inputs=[tokens_input, casing_input], outputs=result)\n",
    "\n",
    "# default lr = 0.001, beta_1=0.9\n",
    "adam = Adam(lr=0.001, beta_1=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "# Если вам удалось разбить предложения на группы одинаковой длины здесь имеет смысл выдвавть батчи из предложений одинаковой \n",
    "# длины размера не более BATCH_SIZE (подобранного так, чтобы батч влезал в память вашей видеокарты)\n",
    "def iterate_minibatches(dataset):   \n",
    "    for sentence in dataset:\n",
    "        tokens, casing, labels = sentence     \n",
    "            \n",
    "        labels = np.expand_dims(labels, -1) \n",
    "        yield np.asarray([tokens]), np.asarray([casing]), np.asarray([labels])\n",
    "\n",
    "# Здесь, соответсвенно, тоже нужно адаптировать код для батчей разной длины        \n",
    "def tag_dataset(dataset):\n",
    "    predicted_labels = []\n",
    "    correct_labels = []\n",
    "    for tokens, casing, labels in dataset:\n",
    "        pred = model.predict_on_batch([np.asarray([tokens]), np.asarray([casing])])[0]\n",
    "        pred_labels = [el.tolist().index(max(el)) for el in pred]\n",
    "        predicted_labels.append(pred_labels)\n",
    "        correct_labels.append(labels)\n",
    "        #print(predicted_labels, correct_labels)\n",
    "    return predicted_labels, correct_labels\n",
    "\n",
    "#В качестве метрики лучше посчитать f-меру по \"склеенным\" сущностям. Можно реализовать склейку как здесь \n",
    "# https://github.com/mit-nlp/MITIE/blob/master/tools/ner_conll/conlleval, можно придумать свой разумный способ\n",
    "def compute_accuracy(predictions, correct, padding_label):\n",
    "    \"\"\"\n",
    "    Получаем на вход предсказанные и истинные метки, считаем точность на них. Паддинги при подсчете не учитываем\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    guessed_tokens = 0\n",
    "    for guessed_sentence, correct_sentence in zip(predictions, correct):\n",
    "        #print(guessed_sentence, correct_sentence)\n",
    "        assert (len(guessed_sentence) == len(correct_sentence)), \"Guessed and correct sentences do not match\"\n",
    "        for j in range(len(guessed_sentence)):\n",
    "            if correct_sentence[j] != padding_label:\n",
    "                total_tokens += 1\n",
    "                if guessed_sentence[j] == correct_sentence[j]:\n",
    "                    guessed_tokens += 1\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        accuracy = float(guessed_tokens) / total_tokens\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "number_of_epochs = 1\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "print(\"%d train sentences\" % len(train_data))\n",
    "print(\"%d dev sentences\" % len(dev_data))\n",
    "print(\"%d test sentences\" % len(test_data))\n",
    "\n",
    "padding_label = label2idx['PADDING_LABEL']\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    for batch in iterate_minibatches(train_data):\n",
    "        #print(batch)\n",
    "        tokens, casing, labels = batch       \n",
    "        model.train_on_batch([tokens, casing], labels)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "               \n",
    "    #Train Dataset       \n",
    "    start_time = time.time()  \n",
    "    print(\"================================== Train Data ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(train_data)        \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "    #Dev Dataset \n",
    "    print(\"================================== Dev Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(dev_data)  \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "    #Test Dataset \n",
    "    #state-of-the-art f-мера~0.91 на test\n",
    "    print(\"================================== Test Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(test_data)  \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "        \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перечислим еще раз все идеи для улучшения, которые можно реализовать\n",
    "# 1. Ограничить работу с эмбеддингами только теми словами, которые встречаются в корпусе. Это ускоряет быстродействие,\n",
    "# но, разумеется, не применимо, если применение сети предполагается на неизвестных данных\n",
    "# 2. Сделать трейн-тест сплит удобного вида)\n",
    "# 3. Перейти от 8 типов капитализации к шаблонам общего вида\n",
    "# 4. Паддить предложения не по одному паддингу с каэдой стороны, а до следующего 2^n + 1\n",
    "# 5. При условии реализации пункта 4 добиться адекватной работы с батчами - добиться, чтобы iterate_minibatches выдавала\n",
    "# батчи из BATCH_SIZE предложений одинакового размера(имеет смысл только если есть желание настроить работу сетки на GPU)\n",
    "# 6. Добавить в сеть символьную часть - подавать эмбеддинги CHAR_SIZE первых символов каждого токена LSTM (или CNN), \n",
    "# результат применения которого конкатенировать со словоформенными эмбеддингами. То же самое сделать с CHAR_SIZE\n",
    "# последними символами\n",
    "# 7. Провести другие эксперименты с архитектурой - сделать LSTM многослойным, заменить на GRU или CNN и т. п.\n",
    "# 8. Добавить в эвалюейшн подсчет ф-меры со \"склейкой\" сущностей\n",
    "# Если сделать все получится близакая к state-of-the-art сетка - на test f-мера >= 0.905"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
