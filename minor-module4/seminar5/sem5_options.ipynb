{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение опций \n",
    "Нашей задачей будет создание набора опций, каждая из которых должна быть обучена достигать определенные состояния в задаче такси. Для обучения мы будем использовать QLearningAgent, которого мы написали на одном из прошлых семинаров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# импортируем файлы и создаем окружение\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "environment = gym.make('Taxi-v2')\n",
    "environment.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем классс для Q-агента\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, gamma, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._q_values = defaultdict(lambda: defaultdict(lambda: 0))  # when called, non-existent values appear as zeros\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "        \"\"\"\n",
    "        return self._q_values[state][action]\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        \"\"\"\n",
    "          Sets the Qvalue for [state,action] to the given value\n",
    "        \"\"\"\n",
    "        self._q_values[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.\n",
    "        \"\"\"\n",
    "\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max([self.get_q_value(state, action) for action in possible_actions])\n",
    "        return value\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.\n",
    "\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "\n",
    "        for action in possible_actions:\n",
    "            if best_action is None:\n",
    "                best_action = action\n",
    "            elif self.get_q_value(state, action) > self.get_q_value(state, best_action):\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state, including exploration.\n",
    "\n",
    "          With probability self.epsilon, we should take a random action.\n",
    "          otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # если в текущей ситуации нет возможных действий - возвращаем None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = random.choice(possible_actions)\n",
    "        else:\n",
    "            action = self.get_policy(state)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        t = self.alpha * (reward + self.gamma * self.get_value(next_state) - self.get_q_value(state, action))\n",
    "        reference_qvalue = self.get_q_value(state, action) + t\n",
    "        self.set_q_value(state, action, reference_qvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 \n",
    "Разберемся как реализована среда Taxi: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
    "\n",
    "Создадим 4 окружения аналогичных Taxi, в которых целью агента будет достижение одной из точек: R, G, B, Y соответственно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiStepWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, target_id, target_reward):\n",
    "        super().__init__(env)\n",
    "        self._target = target_id\n",
    "        self._target_reward = target_reward\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        # получаем изначальные параметры (state, reward, _, obs), которые передает среда, используя метод step \n",
    "        # проверяем является ли полученнуе состояние завершающим для нашего модифицированного окружения\n",
    "        # изменяем вознаграждение (reward) и флаг завершения эпизода (is_done)\n",
    "        # за каждое действие будем давать вознаграждение -1, за достижение цели - self._target_reward\n",
    "        # Ваш код здесь\n",
    "\n",
    "        return state, reward, is_done, obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим нашу обертку (wrapper), используя случайную стратегию.  Порядок точек должен быть  R, G, Y, B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "state:4 reward:50\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[43mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "state:94 reward:50\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state:411 reward:50\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "state:471 reward:50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for target in range(4):\n",
    "\n",
    "\n",
    "    # создаем окружение с заданным целевым состоянием\n",
    "    # Ваш код здесь\n",
    "\n",
    "    \n",
    "    # применяем случайную стратегию, пока эпизод не завершится\n",
    "    # Ваш код здесь\n",
    "\n",
    "\n",
    "    wrapped_env.render()\n",
    "    print(\"state:{s} reward:{r}\\n\".format(**locals()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# воспользуемся методом play_and_train, который мы реализовали на прошлом семинаре\n",
    "def play_and_train(env, agent, t_max=10 ** 4):\n",
    "    total_discounted_reward = 0.0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, next_s, r)\n",
    "        s = next_s\n",
    "        total_discounted_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_discounted_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "1. Обучим агентов на созданных нами окружениях.\n",
    "2. Создадим упрощенный вариант опций, каждая опция будет иметь стратегию, множество начальных состояний и множество конечных состояний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = environment.action_space.n\n",
    "\n",
    "# параметры, которые будут использовать агенты\n",
    "params = {\"alpha\": 0.1, \"epsilon\": 0.1, \"gamma\": 0.99, \"get_legal_actions\": lambda s: range(4)}\n",
    "\n",
    "# создаем агентов \n",
    "agents_for_options = [QLearningAgent(**params) for _ in range(4)]\n",
    "\n",
    "for index in range(4):\n",
    "\n",
    "\n",
    "    # создаем окружение с заданным целевым состоянием\n",
    "    # Ваш код здесь\n",
    "\n",
    "\n",
    "\n",
    "    # используя созданных окружения обучаем на них агентов\n",
    "    # Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем класс опции\n",
    "class Option:\n",
    "    def __init__(self, policy, termination_prob, initial):\n",
    "        self.policy = policy\n",
    "        self.termination_prob = termination_prob\n",
    "        self.initial_states = initial\n",
    "\n",
    "    def can_start(self, state):\n",
    "        return state in self.initial_states\n",
    "\n",
    "    def terminate(self, state):\n",
    "        return random.random() <= self.termination_prob[state]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.policy.get_action(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = []\n",
    "for index, agent in enumerate(agents_for_options):\n",
    "\n",
    "\n",
    "    # Создаем словарь termination_prob, в котором каждому состоянию окружения нужно задать вероятность завершения опции\n",
    "    # В нашем случае зададим 1.0 или 0.0, в зависимости от состояния\n",
    "    # Ваш код здесь\n",
    "\n",
    "\n",
    "\n",
    "    # Создаем множество (set) initial, в которое добавляем все состояния, из которых опция может быть вызвана (все кроме целевых)\n",
    "    # Ваш код здесь\n",
    "\n",
    "    options.append(Option(policy=agent, termination_prob=termination_prob, initial=initial))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Напишем функцию, которая будет запускать опцию и возвращать дисконтированное вознаграждение, опираясь на число совершенных действий\n",
    "$$ R = r_{1} + \\gamma r_{2} + \\gamma^{2} r_{3} + \\dots + \\gamma^{t-1}r_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_option(s, option, gamma, env, debug=False):\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "\n",
    "\n",
    "    # Если опция не может быть запущена - возвращаем None\n",
    "    # Ваш код здесь\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Взаимодействуем со средой пока опция (используем методм terminate) или окружение не завершится\n",
    "    # Считаем дисконтированное вознаграждение reward (используем steps)\n",
    "    # Также добавим render окружения, если включен флаг - debug.\n",
    "    # Ваш код здесь\n",
    "\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "Option terminated\n",
      "SMDP reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "# проверим работу метода\n",
    "env = gym.make('Taxi-v2')\n",
    "s = env.reset()\n",
    "\n",
    "r = apply_option(s, options[0], 0.99, env, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что все хорошо, но мы забыли рассмотреть вариант, когда пассажир может находиться в такси! Переведем среду в состояние, где пассажира мы уже подобрали и посмотрим, как ведет себя  одна из опций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c86f26590ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m499\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.unwrapped.s = 499\n",
    "env.render()\n",
    "print(\"\\n\" * 2)\n",
    "r = apply_option(s, options[0], 0.99, env, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Видим, что опции не обучились действовать в такой ситуации. \n",
    "Исправим нашу функцию обучения так, чтобы опции работали корректно для всех возможных состояний среды и сгенерируем их заново."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train_modified(env, agent, t_max=10 ** 4):\n",
    "    # Зададим новую функцию play_and_train, которая в качестве начального состояния выбирает любое состояние среды,\n",
    "    # включая и то, когда пассажир уже находится в такси\n",
    "\n",
    "    total_discounted_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "    # Выбираем случайное состояние среды, включае то, где пассажир уже в машине (используем метод env.uwrapped)\n",
    "    # Ваш код здесь\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, next_s, r)\n",
    "        s = next_s\n",
    "        total_discounted_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_discounted_reward\n",
    "\n",
    "\n",
    "for index in range(4):\n",
    "    for _ in range(5250):\n",
    "        wrapped_env = TaxiStepWrapper(env=environment, target_id=index, target_reward=50)\n",
    "        play_and_train_modified(env=wrapped_env, agent=agents_for_options[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим данную ячейку несколько раз и убедимся, что агент обучился для всех случаев!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "Option terminated\n",
      "SMDP reward: -3.940399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.940399"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = environment\n",
    "s = env.reset()\n",
    "\n",
    "env.unwrapped.s = random.randint(0, 499)\n",
    "apply_option(s, options[0], 0.99, env, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус \n",
    "Реализуйте иерархию, используя элементарные (опции из одного действия) и обученные опции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
