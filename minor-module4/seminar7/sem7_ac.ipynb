{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 7: Actor-Critic\n",
    "\n",
    "\n",
    "## Майнор ВШЭ, 7.03.2019\n",
    "\n",
    "Опрос - https://goo.gl/forms/WVk4o7hd8qjGzCOp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре будем рассматривать простой эксперимент с алгоритмом Actor-Critic.\n",
    "\n",
    "Рассмотрим задачу с маятиником - здесь количество действий не ограничено!\n",
    "\n",
    "![Маятник](https://cdn-images-1.medium.com/max/1600/1*J_oEx0kpBpwXoVmRytn6qg.gif)\n",
    "\n",
    "Здесь невозможно правильно определить функцию $Q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Модель Actor-Critic](https://cdn-images-1.medium.com/max/1600/1*-GfRVLWhcuSYhG25rN0IbA.png)\n",
    "\n",
    "\n",
    "Модель актор-критик имеет два отдельных аппроксиматора: первый используется для предсказания того, какое действие предпринять, учитывая текущее состояние среды, и второй - чтобы найти значение полезности действия/состояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Модель актора\n",
    "\n",
    "Начнем с импортирования всех необходимых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестировать нашу модель будем на простой задаче с перевернутым маятником."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Создаем окружение Pendulum - смотрим как оно выглядит\n",
    "# env = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с определения модели актера. Цель модели состоит в том, чтобы, учитывая текущее состояние среды, определить наилучшее действие. Для начала будем использовать несколько полносвязных слоев, отображающих наблюдение среды в точку в пространстве среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_model(env):\n",
    "    ### Задаем простую модель - 3-4 полносвязных слоя\n",
    "    # state_input = \n",
    "    # output = \n",
    "\n",
    "    model = Model(input=state_input, output=output)\n",
    "    adam  = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mse\", optimizer=adam)\n",
    "    return state_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы хотим определить, какое изменение параметров (в модели актора) приведет к наибольшему увеличению значения Q (предсказанному моделью критика)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_state_input, actor_model = create_actor_model(env)\n",
    "_, target_actor_model = create_actor_model(env)\n",
    "\n",
    "# where we will feed de/dC (from critic)\n",
    "actor_critic_grad = tf.placeholder(tf.float32, [None, env.action_space.shape[0]]) \n",
    "\n",
    "actor_model_weights = actor_model.trainable_weights\n",
    "# dC/dA (from actor)\n",
    "# Вычисялем градиент и передаем его функции оптимизации\n",
    "# actor_grads = \n",
    "# grads = \n",
    "# optimize = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Модель критика\n",
    "\n",
    "Критик нужен для того, чтобы по состоянию окружающей среды и совершенному действию в качестве входных данных рассчитать соответствующую оценку. Построим его модель с помощью нескольких полвносвязных слоев с одним объединением перед окончательным предсказанием Q-значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic_model(env):\n",
    "    ### Задаем простую модель - 2 полносвязных слоя для состояний и 1 слой для действий\n",
    "    # state_input = \n",
    "\n",
    "    # action_input = \n",
    "\n",
    "    ### Объдиняем оба выхода и далее добавляем еще пару полносвязных слоев\n",
    "    # merged    =\n",
    "    #output = \n",
    "    \n",
    "    ### Все собираем в кучу\n",
    "    # model  = \n",
    "\n",
    "    adam  = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mse\", optimizer=adam)\n",
    "    return state_input, action_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужны ссылки как на входы действий, так и на входы по состоянию, поскольку нам нужно использовать их при обновлении для сети актора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_state_input, critic_action_input, critic_model = create_critic_model(env)\n",
    "_, _, target_critic_model = create_critic_model(env)\n",
    "\n",
    "# Вычисялем градиент критика\n",
    "# where we calcaulte de/dC for feeding above\n",
    "# critic_grads = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "# Initialize for later gradient calculations\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Определяем метод обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим метод обучения по памяти. Мы просто находим дисконтированное будущее вознаграждение и обучаемся на этом. Обучение проводим на паре состояние/действие и используем target_critic_model для прогнозирования будущей награды. Для актора мы уже определили как градиенты будут работать в сети, и теперь просто должны вызвать их с действиями и состояниями, которые берем из памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .95\n",
    "\n",
    "def train(sess, memory):\n",
    "    batch_size = 32\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    rewards = []\n",
    "    ### Выбираем случай батч\n",
    "    # samples = \n",
    "  \n",
    "    ### Обучение модели актора\n",
    "    for sample in samples:\n",
    "        cur_state, action, reward, new_state, _ = sample\n",
    "        ### Предсказываем действие\n",
    "        # predicted_action = \n",
    "        ### Вычисляем градиент в сессии\n",
    "        # grads = sess.run(...)[0]\n",
    "\n",
    "        ### Делаем шаг оптимизациив сессии\n",
    "        # sess.run(....)\n",
    "        \n",
    "    ### Обучение модели критика\n",
    "    for sample in samples:\n",
    "        cur_state, action, reward, new_state, done = sample\n",
    "        if not done:\n",
    "            ### Предсказываем действие\n",
    "            # target_action = \n",
    "            ### Предсказываем будущее вознаграждение\n",
    "            # future_reward = \n",
    "            ### Считаем будущее вознаграждение\n",
    "            # reward ...\n",
    "        ### Обучаем модель критика\n",
    "        #critic_model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой модели мы используем целевую сеть для улучшения сходимости. Мы должны обновлять ее веса на каждом временном шаге. Однако мы делаем это с меньшей частотой ,которая задается параметром $\\tau$. Проделываем эту операцию как для актера так и для критика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau   = .125\n",
    "\n",
    "def update_target():\n",
    "    actor_model_weights  = actor_model.get_weights()\n",
    "    actor_target_weights = target_actor_model.get_weights()\n",
    "\n",
    "    for i in range(len(actor_target_weights)):\n",
    "        actor_target_weights[i] = (1-tau)*actor_model_weights[i]+tau*actor_target_weights[i]\n",
    "    target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "\n",
    "    critic_model_weights  = critic_model.get_weights()\n",
    "    critic_target_weights = target_critic_model.get_weights()\n",
    "\n",
    "    for i in range(len(critic_target_weights)):\n",
    "        critic_target_weights[i] = (1-tau)*critic_model_weights[i]+tau*critic_target_weights[i]\n",
    "    critic_target_model.set_weights(critic_target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Проводим эксперименты.\n",
    "\n",
    "Записываем полный цикл обучения: запоминиаем статистику и проводим на ней обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10000\n",
    "trial_len  = 500\n",
    "epsilon = 1.0\n",
    "epsilon_decay = .995\n",
    "\n",
    "cur_state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "while True:\n",
    "    env.render()\n",
    "    cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "    \n",
    "    epsilon *= epsilon_decay\n",
    "    ### Реализуем epsilon-жадную стратегию\n",
    "    # action = \n",
    "    \n",
    "    action = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "    ### Сохраняем в память данные\n",
    "    # memory...\n",
    "    train(sess, memory)\n",
    "    \n",
    "    update_target()\n",
    "\n",
    "    cur_state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте график вознаграждения в зависимости от номера эпизода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
