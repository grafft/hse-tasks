{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 4: аппроксимация Q-функции\n",
    "\n",
    "## Майнор ВШЭ, 14.02.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке мы будем использовать библиотеку __tensorflow__ для обучения нейронной сети, хотя можно использвать и любую другую библиотеку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем тестировать наши модели на классической задаче с перевернутым маятником."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f690579940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEmpJREFUeJzt3X+s3fV93/HnazaBLMlqCBfk2WYmrbeGTouhd8QR00QhbYFVM5WaCTY1KEK6mUSkRI22QietiTSkVlrDFm1DcQuNU2UhjCTDQqyp5xBV+SMQkziOjUO5Saz41h6+WYAki8Zm8t4f53PDmTm+9/j+8PX99PmQjs73+/l+zve8P/jwul9/7vfjk6pCktSfv7baBUiSVoYBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUqRUL+CQ3JXk2yXSSu1fqfSRJo2Ul7oNPsg74C+CXgRngK8DtVfXMsr+ZJGmklbqCvxaYrqpvV9X/AR4Cdq7Qe0mSRli/QufdBBwb2p8B3n6mzpdeemlt3bp1hUqRpLXn6NGjfO9738tSzrFSAT+qqP9vLijJFDAFcMUVV7B///4VKkWS1p7Jyckln2OlpmhmgC1D+5uB48MdqmpXVU1W1eTExMQKlSFJf3WtVMB/BdiW5MokrwNuA/as0HtJkkZYkSmaqjqV5H3A54F1wINVdXgl3kuSNNpKzcFTVY8Dj6/U+SVJ83MlqyR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekTi3pK/uSHAV+CLwCnKqqySSXAJ8GtgJHgX9SVS8srUxJ0tlajiv4X6qq7VU12fbvBvZV1TZgX9uXJJ1jKzFFsxPY3bZ3A7euwHtIkhaw1IAv4M+SPJ1kqrVdXlUnANrzZUt8D0nSIixpDh64rqqOJ7kM2Jvkm+O+sP1AmAK44oorlliGJOl0S7qCr6rj7fkk8DngWuD5JBsB2vPJM7x2V1VNVtXkxMTEUsqQJI2w6IBP8oYkb5rbBn4FOATsAe5o3e4AHl1qkZKks7eUKZrLgc8lmTvPf66qP03yFeDhJHcC3wXetfQyJUlna9EBX1XfBt42ov1/AjcupShJ0tK5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnq1IIBn+TBJCeTHBpquyTJ3iTPteeLW3uSfDTJdJKDSa5ZyeIlSWc2zhX8x4GbTmu7G9hXVduAfW0f4GZgW3tMAfcvT5mSpLO1YMBX1Z8D3z+teSewu23vBm4dav9EDXwZ2JBk43IVK0ka32Ln4C+vqhMA7fmy1r4JODbUb6a1vUaSqST7k+yfnZ1dZBmSpDNZ7l+yZkRbjepYVbuqarKqJicmJpa5DEnSYgP++bmpl/Z8srXPAFuG+m0Gji++PEnSYi024PcAd7TtO4BHh9rf3e6m2QG8NDeVI0k6t9Yv1CHJp4DrgUuTzAC/C/we8HCSO4HvAu9q3R8HbgGmgR8D71mBmiVJY1gw4Kvq9jMcunFE3wLuWmpRkqSlcyWrJHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROLRjwSR5McjLJoaG2DyX5yyQH2uOWoWP3JJlO8mySX12pwiVJ8xvnCv7jwE0j2u+rqu3t8ThAkquA24BfaK/5T0nWLVexkqTxLRjwVfXnwPfHPN9O4KGqermqvgNMA9cuoT5J0iItZQ7+fUkOtimci1vbJuDYUJ+Z1vYaSaaS7E+yf3Z2dgllSJJGWWzA3w/8LLAdOAH8QWvPiL416gRVtauqJqtqcmJiYpFlSJLOZFEBX1XPV9UrVfUT4A95dRpmBtgy1HUzcHxpJUqSFmNRAZ9k49DurwNzd9jsAW5LcmGSK4FtwFNLK1GStBjrF+qQ5FPA9cClSWaA3wWuT7KdwfTLUeC9AFV1OMnDwDPAKeCuqnplZUqXJM1nwYCvqttHND8wT/97gXuXUpQkaelcySpJnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6teBtklLvnt713pHtvzj1sXNcibS8vIKXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6tWDAJ9mS5IkkR5IcTvL+1n5Jkr1JnmvPF7f2JPlokukkB5Ncs9KDkCS91jhX8KeAD1bVW4EdwF1JrgLuBvZV1TZgX9sHuBnY1h5TwP3LXrUkaUELBnxVnaiqr7btHwJHgE3ATmB367YbuLVt7wQ+UQNfBjYk2bjslUuS5nVWc/BJtgJXA08Cl1fVCRj8EAAua902AceGXjbT2k4/11SS/Un2z87Onn3lkqR5jR3wSd4IfAb4QFX9YL6uI9rqNQ1Vu6pqsqomJyYmxi1DkjSmsQI+yQUMwv2TVfXZ1vz83NRLez7Z2meALUMv3wwcX55yJUnjGucumgAPAEeq6iNDh/YAd7TtO4BHh9rf3e6m2QG8NDeVI0k6d8b5yr7rgN8EvpHkQGv7HeD3gIeT3Al8F3hXO/Y4cAswDfwYeM+yVixJGsuCAV9VX2L0vDrAjSP6F3DXEuuSJC2RK1klqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4KURfnHqY6tdgrRkBrwkdWqcL93ekuSJJEeSHE7y/tb+oSR/meRAe9wy9Jp7kkwneTbJr67kACRJo43zpdungA9W1VeTvAl4Osneduy+qvq3w52TXAXcBvwC8DeB/57kb1fVK8tZuCRpfgtewVfViar6atv+IXAE2DTPS3YCD1XVy1X1HWAauHY5ipUkje+s5uCTbAWuBp5sTe9LcjDJg0kubm2bgGNDL5th/h8IkqQVMHbAJ3kj8BngA1X1A+B+4GeB7cAJ4A/muo54eY0431SS/Un2z87OnnXhkqT5jRXwSS5gEO6frKrPAlTV81X1SlX9BPhDXp2GmQG2DL18M3D89HNW1a6qmqyqyYmJiaWMQZI0wjh30QR4ADhSVR8Zat841O3XgUNtew9wW5ILk1wJbAOeWr6SJUnjGOcumuuA3wS+keRAa/sd4PYk2xlMvxwF3gtQVYeTPAw8w+AOnLu8g0aSzr0FA76qvsToefXH53nNvcC9S6hLkrRErmSVpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAGvLiUZ+7ESr5fOBwa8JHVqnC/8kLr32Impn27/2sZdq1iJtHy8gtdfecPhPmpfWqsMeEnq1Dhfun1RkqeSfD3J4SQfbu1XJnkyyXNJPp3kda39wrY/3Y5vXdkhSJJGGecK/mXghqp6G7AduCnJDuD3gfuqahvwAnBn638n8EJV/RxwX+snnbdOn3N3Dl69GOdLtwv4Udu9oD0KuAH4p619N/Ah4H5gZ9sGeAT4D0nSziOddybfuwt4NdQ/tGqVSMtrrDn4JOuSHABOAnuBbwEvVtWp1mUG2NS2NwHHANrxl4A3L2fRkqSFjRXwVfVKVW0HNgPXAm8d1a09j1r58Zqr9yRTSfYn2T87OztuvZKkMZ3VXTRV9SLwRWAHsCHJ3BTPZuB4254BtgC04z8DfH/EuXZV1WRVTU5MTCyueknSGY1zF81Ekg1t+/XAO4EjwBPAb7RudwCPtu09bZ92/AvOv0vSuTfOStaNwO4k6xj8QHi4qh5L8gzwUJJ/A3wNeKD1fwD4kyTTDK7cb1uBuiVJCxjnLpqDwNUj2r/NYD7+9Pb/DbxrWaqTJC2aK1klqVMGvCR1yoCXpE75zwWrS964JXkFL0ndMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6Nc6Xbl+U5KkkX09yOMmHW/vHk3wnyYH22N7ak+SjSaaTHExyzUoPQpL0WuP8e/AvAzdU1Y+SXAB8Kcl/a8f+RVU9clr/m4Ft7fF24P72LEk6hxa8gq+BH7XdC9pjvm9T2Al8or3uy8CGJBuXXqok6WyMNQefZF2SA8BJYG9VPdkO3dumYe5LcmFr2wQcG3r5TGuTJJ1DYwV8Vb1SVduBzcC1Sf4ucA/w88DfBy4Bfrt1z6hTnN6QZCrJ/iT7Z2dnF1W8JOnMzuoumqp6EfgicFNVnWjTMC8Dfwxc27rNAFuGXrYZOD7iXLuqarKqJicmJhZVvCTpzMa5i2YiyYa2/XrgncA35+bVkwS4FTjUXrIHeHe7m2YH8FJVnViR6iVJZzTOXTQbgd1J1jH4gfBwVT2W5AtJJhhMyRwA/nnr/zhwCzAN/Bh4z/KXLUlayIIBX1UHgatHtN9whv4F3LX00iRJS+FKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTYwd8knVJvpbksbZ/ZZInkzyX5NNJXtfaL2z70+341pUpXZI0n7O5gn8/cGRo//eB+6pqG/ACcGdrvxN4oap+Driv9ZMknWNjBXySzcA/Av6o7Qe4AXikddkN3Nq2d7Z92vEbW39J0jm0fsx+/w74l8Cb2v6bgRer6lTbnwE2te1NwDGAqjqV5KXW/3vDJ0wyBUy13ZeTHFrUCM5/l3La2DvR67ig37E5rrXlbyWZqqpdiz3BggGf5NeAk1X1dJLr55pHdK0xjr3aMCh6V3uP/VU1OVbFa0yvY+t1XNDv2BzX2pNkPy0nF2OcK/jrgH+c5BbgIuBvMLii35BkfbuK3wwcb/1ngC3ATJL1wM8A319sgZKkxVlwDr6q7qmqzVW1FbgN+EJV/TPgCeA3Wrc7gEfb9p62Tzv+hap6zRW8JGllLeU++N8GfivJNIM59gda+wPAm1v7bwF3j3GuRf8VZA3odWy9jgv6HZvjWnuWNLZ4cS1JfXIlqyR1atUDPslNSZ5tK1/Hmc45ryR5MMnJ4ds8k1ySZG9b5bs3ycWtPUk+2sZ6MMk1q1f5/JJsSfJEkiNJDid5f2tf02NLclGSp5J8vY3rw629i5XZva44T3I0yTeSHGh3lqz5zyJAkg1JHknyzfb/2juWc1yrGvBJ1gH/EbgZuAq4PclVq1nTInwcuOm0truBfW2V7z5e/T3EzcC29pgC7j9HNS7GKeCDVfVWYAdwV/uzWetjexm4oareBmwHbkqyg35WZve84vyXqmr70C2Ra/2zCPDvgT+tqp8H3sbgz275xlVVq/YA3gF8fmj/HuCe1axpkePYChwa2n8W2Ni2NwLPtu2PAbeP6ne+PxjcJfXLPY0N+OvAV4G3M1gos761//RzCXweeEfbXt/6ZbVrP8N4NrdAuAF4jMGalDU/rlbjUeDS09rW9GeRwS3n3zn9v/tyjmu1p2h+uuq1GV4Ru5ZdXlUnANrzZa19TY63/fX9auBJOhhbm8Y4AJwE9gLfYsyV2cDcyuzz0dyK85+0/bFXnHN+jwsGiyX/LMnTbRU8rP3P4luAWeCP27TaHyV5A8s4rtUO+LFWvXZkzY03yRuBzwAfqKofzNd1RNt5ObaqeqWqtjO44r0WeOuobu15TYwrQyvOh5tHdF1T4xpyXVVdw2Ca4q4k/3CevmtlbOuBa4D7q+pq4H8x/23lZz2u1Q74uVWvc4ZXxK5lzyfZCNCeT7b2NTXeJBcwCPdPVtVnW3MXYwOoqheBLzL4HcOGtvIaRq/M5jxfmT234vwo8BCDaZqfrjhvfdbiuACoquPt+STwOQY/mNf6Z3EGmKmqJ9v+IwwCf9nGtdoB/xVgW/tN/+sYrJTds8o1LYfh1bynr/J9d/tt+A7gpbm/ip1vkoTBorUjVfWRoUNremxJJpJsaNuvB97J4Bdba3pldnW84jzJG5K8aW4b+BXgEGv8s1hV/wM4luTvtKYbgWdYznGdB79ouAX4CwbzoP9qtetZRP2fAk4A/5fBT9g7Gcxl7gOea8+XtL5hcNfQt4BvAJOrXf884/oHDP76dxA40B63rPWxAX8P+Fob1yHgX7f2twBPAdPAfwEubO0Xtf3pdvwtqz2GMcZ4PfBYL+NqY/h6exyey4m1/llstW4H9rfP438FLl7OcbmSVZI6tdpTNJKkFWLAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUqf8H/yN9fY1j9aIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (4,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions, state_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокое Q-обучение: построение сети\n",
    "\n",
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "\n",
    "![img](qlearning_scheme.png)\n",
    "\n",
    "Для начала попробуйте использовать только полносвязные слои (__L.Dense__) и линейные активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными. Начиние с сети с 1-2 скрытыми слоями и 100-200 нейронами, а затем усложняйте сеть пока не будет достигнут приличный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "\n",
    "network.add(L.Dense(100, activation=\"relu\"))\n",
    "network.add(L.Dense(20, activation=\"relu\"))\n",
    "# network.add(L.Dense(50, activation=\"relu\"))\n",
    "network.add(L.Dense(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    ### Ваш код здесь - нужно выбрать действия e-жадно ###\n",
    "    action = None\n",
    "    if epsilon > random.random():\n",
    "        action = random.choice(range(n_actions))\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "        \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-обучение через градиентный спуск\n",
    "\n",
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n",
    "\n",
    "Основная тонкость состоит в использовани  $Q_{-}(s',a')$. Эта таже самая функция, что и $Q_{\\theta}$, которая является выходомо нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Ддля этого используется функция `tf.stop_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "# compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss to minimize\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 13.740\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 13.870\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 13.980\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 13.440\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 17.720\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 12.510\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 14.190\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 13.290\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 27.010\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 23.090\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 31.700\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 16.320\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 21.910\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 41.120\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 39.390\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 39.020\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 50.670\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 49.830\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 55.080\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 68.760\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 87.500\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 145.010\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 166.690\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 161.580\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 162.810\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 248.800\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 169.710\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 191.750\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 190.730\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 184.270\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 254.740\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 197.930\tepsilon = 0.366\n",
      "epoch #32\tmean reward = 201.710\tepsilon = 0.362\n",
      "epoch #33\tmean reward = 233.260\tepsilon = 0.359\n",
      "epoch #34\tmean reward = 233.800\tepsilon = 0.355\n",
      "epoch #35\tmean reward = 235.940\tepsilon = 0.352\n",
      "epoch #36\tmean reward = 224.150\tepsilon = 0.348\n",
      "epoch #37\tmean reward = 325.910\tepsilon = 0.345\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация результатов\n",
    "\n",
    "\n",
    "* __mean reward__  - средне вознаграждеие за эпизод. в Случае корреткной реализации, этот показатель будет низким 10 эпох и только затем будет возрастать и сойдется на 50-100 шагов в зависииости от архитектуры сети.\n",
    "* Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsilon$.\n",
    "* __epsilon__ обеспечивает стремление агента исследовать среду. Можно искусственно увеличвать малые значения $\\epsilon$ при низких результатаз до 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись роликов\n",
    "\n",
    "Можно использовать `gym.wrappers.Monitor` для записи сессий агента. \n",
    "\n",
    "Для финальной пробы агента, мы будем ставить  epsilon=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.2788.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
