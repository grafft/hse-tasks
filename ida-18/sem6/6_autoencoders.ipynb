{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автокодировщики\n",
    "\n",
    "На этом занятии будем обучать глубокие автокодировщики на примере изображений человеческих лиц [lfw dataset](http://vis-www.cs.umass.edu/lfw/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Начинаем работу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:38:56.825485Z",
     "start_time": "2018-01-04T16:38:46.131894Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras, keras.layers as L, keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lfw_dataset import load_lfw_dataset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import download_utils\n",
    "import keras_utils\n",
    "import numpy as np\n",
    "from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "Оригинальные ссылки:\n",
    "- http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n",
    "- http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
    "- http://vis-www.cs.umass.edu/lfw/lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:39:21.680162Z",
     "start_time": "2018-01-04T16:39:01.554782Z"
    }
   },
   "outputs": [],
   "source": [
    "# load images\n",
    "X, attr = load_lfw_dataset(use_raw=True, dimx=32, dimy=32)\n",
    "IMG_SHAPE = X.shape[1:]\n",
    "\n",
    "# center images\n",
    "X = X.astype('float32') / 255.0 - 0.5\n",
    "\n",
    "# split\n",
    "X_train, X_test = train_test_split(X, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:39:36.649891Z",
     "start_time": "2018-01-04T16:39:36.646605Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(x):\n",
    "    plt.imshow(np.clip(x + 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:39:39.028360Z",
     "start_time": "2018-01-04T16:39:38.258425Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('sample images')\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    show_image(X[i])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"attr shape:\", attr.shape)\n",
    "\n",
    "# try to free memory\n",
    "del X\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура автокодировщика\n",
    "\n",
    "Будем строить архитектуру сети как две последовательные модели в Keras: кодировщик и декодировщик. Затем применяем обычное API для обучения моделей.\n",
    "\n",
    "<img src=\"autoencoder.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первый шаг: метод главных компонент\n",
    "\n",
    "Метод главных компонент (PCA) - популярный метод сокращения размерности.\n",
    "\n",
    "\n",
    "PCA пытается декомопзировать матрицу объекты-признаки на две матрице меньшего размера $W$ и $\\hat W$, минимизируя средний квадрат ошибки:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - объектная матрица (**центрированная**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - матрица прямой трансформации;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - матрица обратной трансформации;\n",
    "- $n$ примеров, $m$ - начальная размерность, $d$ - итоговая размерность;\n",
    "\n",
    "Геометрическая интерпретация: мы хотим найти $d$ осей по которым наблюдается наибольшая дисперсия.\n",
    "\n",
    "<img src=\"pca.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "PCA можно рассмаривать как особый случай автокодировщика.\n",
    "\n",
    "* __Encoder__: X -> Dense(d units) -> код\n",
    "* __Decoder__: код -> Dense(m units) -> X\n",
    "\n",
    "Здесь Dense это полносвязаный слой с линейной активацией:   $f(X) = W \\cdot X + \\vec b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:39:42.058684Z",
     "start_time": "2018-01-04T16:39:42.046303Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_pca_autoencoder(img_shape, code_size):\n",
    "    \"\"\"\n",
    "    Here we define a simple linear autoencoder as described above.\n",
    "    We also flatten and un-flatten data to be compatible with image shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder = keras.models.Sequential()\n",
    "    ## ВАШ КОД: добавляем простые слои\n",
    "\n",
    "    decoder = keras.models.Sequential()\n",
    "    ## ВАШ КОД: добавляем обратные слои\n",
    "    \n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем в одну модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:41:04.366409Z",
     "start_time": "2018-01-04T16:40:45.919042Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = reset_tf_session()\n",
    "\n",
    "encoder, decoder = build_pca_autoencoder(IMG_SHAPE, code_size=32)\n",
    "\n",
    "inp = L.Input(IMG_SHAPE)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)\n",
    "autoencoder.compile(optimizer='adamax', loss='mse')\n",
    "\n",
    "# Обучаем модель! epochs=15, callbaback=[keras_utils.TqdmProgressCallback()], validation, verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:41:11.747674Z",
     "start_time": "2018-01-04T16:41:11.730725Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(img,encoder,decoder):\n",
    "    \"\"\"Draws original, encoded and decoded images\"\"\"\n",
    "    code = encoder.predict(img[None])[0]  # img[None] is the same as img[np.newaxis, :]\n",
    "    reco = decoder.predict(code[None])[0]\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Original\")\n",
    "    show_image(img)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Code\")\n",
    "    plt.imshow(code.reshape([code.shape[-1]//2,-1]))\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    show_image(reco)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:41:18.700138Z",
     "start_time": "2018-01-04T16:41:17.026047Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(X_test,X_test,verbose=0)\n",
    "print(\"PCA MSE:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_test[2]\n",
    "visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный автокодировщик\n",
    "\n",
    "Добавим еще большей слоев в модель.\n",
    "\n",
    "## Кодировщик\n",
    "\n",
    "Кодировщик будет довольно стандартный - составляем сверточные слои и взятие максимума для получения представления нужного размера (`code_size`).\n",
    "\n",
    "Будем использовать `activation='elu'`для всех полносвязных и сверточных слоев.\n",
    "\n",
    "Начнем с 4 пар (conv, pool) с ядром (3, 3), `padding='same'` и выходными каналами `32, 64, 128, 256`. Плюс не забываем про вытягивание (`L.Flatten()`) вызода перед последним полносвязным слоем!\n",
    "\n",
    "## Декодировщик\n",
    "Для декодировщика будем использовать \"транспонированную свертку\". \n",
    "\n",
    "Обычный полносвязный слой по части изображения генерирует число (patch -> number). В \"транспонированной свертке\" мы производим обратную операцию (number -> patch), производя \"разворачивание\" сверток кодировщика( см. [this video](https://www.coursera.org/learn/intro-to-deep-learning/lecture/auRqf/a-glimpse-of-other-computer-vision-tasks) с 5:41).\n",
    "\n",
    "<img src=\"transpose_conv.jpg\" style=\"width:60%\">\n",
    "Здесь используется сдвиг 2 для генерации выхода размером 4x4, производя и обратную операцию взятия максимума. .\n",
    "\n",
    "Как это делается в Keras:\n",
    "```python\n",
    "L.Conv2DTranspose(filters=?, kernel_size=(3, 3), strides=2, activation='elu', padding='same')\n",
    "```\n",
    "\n",
    "Будем строить декодер, начиная с полносвязного слоя (помним про reshape для обратной операции к `L.Flatten()`). Затем мы будем поочередно обращать все пары (conv, pool): составляем 4 `L.Conv2DTranspose` слоя с layers with the following numbers of output channelsобратным порядком выходных каналов: `128, 64, 32, 3`. Для последнего  `L.Conv2DTranspose` слоя возьмем `activation=None`, т.к. это итоговая картинка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:43:33.562406Z",
     "start_time": "2018-01-04T16:43:33.426581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's play around with transpose convolution on examples first\n",
    "def test_conv2d_transpose(img_size, filter_size):\n",
    "    print(\"Transpose convolution test for img_size={}, filter_size={}:\".format(img_size, filter_size))\n",
    "    \n",
    "    x = (np.arange(img_size ** 2, dtype=np.float32) + 1).reshape((1, img_size, img_size, 1))\n",
    "    f = (np.ones(filter_size ** 2, dtype=np.float32)).reshape((filter_size, filter_size, 1, 1))\n",
    "\n",
    "    s = reset_tf_session()\n",
    "    \n",
    "    conv = tf.nn.conv2d_transpose(x, f, \n",
    "                                  output_shape=(1, img_size * 2, img_size * 2, 1), \n",
    "                                  strides=[1, 2, 2, 1], \n",
    "                                  padding='SAME')\n",
    "\n",
    "    result = s.run(conv)\n",
    "    print(\"input:\")\n",
    "    print(x[0, :, :, 0])\n",
    "    print(\"filter:\")\n",
    "    print(f[:, :, 0, 0])\n",
    "    print(\"output:\")\n",
    "    print(result[0, :, :, 0])\n",
    "    s.close()\n",
    "        \n",
    "test_conv2d_transpose(img_size=2, filter_size=2)\n",
    "test_conv2d_transpose(img_size=2, filter_size=3)\n",
    "test_conv2d_transpose(img_size=4, filter_size=2)\n",
    "test_conv2d_transpose(img_size=4, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:44:43.582011Z",
     "start_time": "2018-01-04T16:44:43.516283Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_deep_autoencoder(img_shape, code_size):\n",
    "    \"\"\"PCA's deeper brother. See instructions above. Use `code_size` in layer definitions.\"\"\"\n",
    "    H,W,C = img_shape\n",
    "    \n",
    "    # encoder\n",
    "    encoder = keras.models.Sequential()\n",
    "    encoder.add(L.InputLayer(img_shape))\n",
    "    \n",
    "    # ВАШ КОД: строим модель\n",
    "\n",
    "    # decoder\n",
    "    decoder = keras.models.Sequential()\n",
    "    decoder.add(L.InputLayer((code_size,)))\n",
    "    \n",
    "    encoder_prev_shape = (img_shape[0] // 2**4, img_shape[1] // 2**4, 256)\n",
    "    # ВАШ КОД: строим модель\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:44:53.805124Z",
     "start_time": "2018-01-04T16:44:52.846510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check autoencoder shapes along different code_sizes\n",
    "get_dim = lambda layer: np.prod(layer.output_shape[1:])\n",
    "for code_size in [1,8,32,128,512]:\n",
    "    s = reset_tf_session()\n",
    "    encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=code_size)\n",
    "    print(\"Testing code size %i\" % code_size)\n",
    "    assert encoder.output_shape[1:]==(code_size,),\"encoder must output a code of required size\"\n",
    "    assert decoder.output_shape[1:]==IMG_SHAPE,   \"decoder must output an image of valid shape\"\n",
    "    assert len(encoder.trainable_weights)>=6,     \"encoder must contain at least 3 layers\"\n",
    "    assert len(decoder.trainable_weights)>=6,     \"decoder must contain at least 3 layers\"\n",
    "    \n",
    "    for layer in encoder.layers + decoder.layers:\n",
    "        assert get_dim(layer) >= code_size, \"Encoder layer %s is smaller than bottleneck (%i units)\"%(layer.name,get_dim(layer))\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "s = reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:45:16.966538Z",
     "start_time": "2018-01-04T16:45:16.804252Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at encoder and decoder shapes.\n",
    "# Total number of trainable parameters of encoder and decoder should be close.\n",
    "s = reset_tf_session()\n",
    "encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучем нашу модель. Займет порядка **1 часа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:48:32.365157Z",
     "start_time": "2018-01-04T16:46:03.202875Z"
    }
   },
   "outputs": [],
   "source": [
    "s = reset_tf_session()\n",
    "\n",
    "encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\n",
    "\n",
    "# Создаем модель как в примере с PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:48:32.365157Z",
     "start_time": "2018-01-04T16:46:03.202875Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will save model checkpoints here to continue training in case of kernel death\n",
    "model_filename = 'autoencoder.{0:03d}.hdf5'\n",
    "last_finished_epoch = None\n",
    "\n",
    "#### uncomment below to continue training from model checkpoint\n",
    "#### fill `last_finished_epoch` with your latest finished epoch\n",
    "# from keras.models import load_model\n",
    "# s = reset_tf_session()\n",
    "# last_finished_epoch = 4\n",
    "# autoencoder = load_model(model_filename.format(last_finished_epoch))\n",
    "# encoder = autoencoder.layers[1]\n",
    "# decoder = autoencoder.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:48:32.365157Z",
     "start_time": "2018-01-04T16:46:03.202875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Обучаем модель! epochs=25, callbaback=[keras_utils.TqdmProgressCallback()], validation, verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:49:25.084704Z",
     "start_time": "2018-01-04T16:49:23.236568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction_mse = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "print(\"Convolutional autoencoder MSE:\", reconstruction_mse)\n",
    "for i in range(5):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:49:54.273061Z",
     "start_time": "2018-01-04T16:49:54.230656Z"
    }
   },
   "outputs": [],
   "source": [
    "# save trained weights\n",
    "encoder.save_weights(\"encoder.h5\")\n",
    "decoder.save_weights(\"decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автокодировщик для удаления шума\n",
    "\n",
    "рассмтоим одно из приложений авктодировщиков - удаление шума.\n",
    "\n",
    "Как будет работать удаление шума:\n",
    "<img src=\"denoising.jpg\" style=\"width:40%\">\n",
    "\n",
    "Будем исопльзовать ту же архитектуру, но измением то, как будем ее обучать: немного повредим наши данные случайнм образом перед каждой эпохой.\n",
    "\n",
    "Есть много подходов как добавить шум: гауссовский белый шум, перекрытие случайными черными прямоугольниками и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:52:04.861818Z",
     "start_time": "2018-01-04T16:52:04.856134Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_gaussian_noise(X,sigma=0.1):\n",
    "    \"\"\"\n",
    "    adds noise from standard normal distribution with standard deviation sigma\n",
    "    :param X: image tensor of shape [batch,height,width,3]\n",
    "    Returns X + noise.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(scale=sigma, size=X.shape)\n",
    "    return X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:52:06.264119Z",
     "start_time": "2018-01-04T16:52:06.223714Z"
    }
   },
   "outputs": [],
   "source": [
    "# noise tests\n",
    "theoretical_std = (X_train[:100].std()**2 + 0.5**2)**.5\n",
    "our_std = apply_gaussian_noise(X_train[:100],sigma=0.5).std()\n",
    "assert abs(theoretical_std - our_std) < 0.01, \"Standard deviation does not match it's required value. Make sure you use sigma as std.\"\n",
    "assert abs(apply_gaussian_noise(X_train[:100],sigma=0.5).mean() - X_train[:100].mean()) < 0.01, \"Mean has changed. Please add zero-mean noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:52:08.220681Z",
     "start_time": "2018-01-04T16:52:07.737460Z"
    }
   },
   "outputs": [],
   "source": [
    "# test different noise scales\n",
    "plt.subplot(1,4,1)\n",
    "show_image(X_train[0])\n",
    "plt.subplot(1,4,2)\n",
    "show_image(apply_gaussian_noise(X_train[:1],sigma=0.01)[0])\n",
    "plt.subplot(1,4,3)\n",
    "show_image(apply_gaussian_noise(X_train[:1],sigma=0.1)[0])\n",
    "plt.subplot(1,4,4)\n",
    "show_image(apply_gaussian_noise(X_train[:1],sigma=0.5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем ~ **1 часа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:02.667408Z",
     "start_time": "2018-01-04T16:52:31.853874Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = reset_tf_session()\n",
    "\n",
    "# we use bigger code size here for better quality\n",
    "encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=512)\n",
    "assert encoder.output_shape[1:]==(512,), \"encoder must output a code of required size\"\n",
    "\n",
    "inp = L.Input(IMG_SHAPE)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = keras.models.Model(inp, reconstruction)\n",
    "autoencoder.compile('adamax', 'mse')\n",
    "\n",
    "for i in range(25):\n",
    "    print(\"Epoch %i/25, Generating corrupted samples...\"%(i+1))\n",
    "    X_train_noise = apply_gaussian_noise(X_train)\n",
    "    X_test_noise = apply_gaussian_noise(X_test)\n",
    "    \n",
    "    # we continue to train our model with new noise-augmented data\n",
    "    # Обучаем модель! epochs=1, callbaback=[keras_utils.TqdmProgressCallback()], validation, verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:09.059164Z",
     "start_time": "2018-01-04T16:56:06.987995Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_noise = apply_gaussian_noise(X_test)\n",
    "denoising_mse = autoencoder.evaluate(X_test_noise, X_test, verbose=0)\n",
    "print(\"Denoising MSE:\", denoising_mse)\n",
    "for i in range(5):\n",
    "    img = X_test_noise[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск изображений с помощью автокодировщика\n",
    "\n",
    "Поддим автокодирощвику изображение и найдем походие изображения в его скрытом пространстве:\n",
    "\n",
    "<img src=\"similar_images.jpg\" style=\"width:60%\">\n",
    "\n",
    "Для ускорения процесса извлечения будем использовать локльное хеширование векторов кодирощвика ([technique](https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries.html)), что позволит уменьить число потенциальных соседей нашего изображения. Будем искать ближайших простым перебором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:25.988163Z",
     "start_time": "2018-01-04T16:56:25.784071Z"
    }
   },
   "outputs": [],
   "source": [
    "# restore trained encoder weights\n",
    "s = reset_tf_session()\n",
    "encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\n",
    "encoder.load_weights(\"encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:30.368727Z",
     "start_time": "2018-01-04T16:56:29.246409Z"
    }
   },
   "outputs": [],
   "source": [
    "images = X_train\n",
    "codes = encoder.predict(X_train)\n",
    "assert len(codes) == len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:39.396176Z",
     "start_time": "2018-01-04T16:56:39.370156Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.unsupervised import NearestNeighbors\n",
    "nei_clf = NearestNeighbors(metric=\"euclidean\")\n",
    "nei_clf.fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:42.213214Z",
     "start_time": "2018-01-04T16:56:42.206902Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "    assert image.ndim==3,\"image must be [batch,height,width,3]\"\n",
    "\n",
    "    code = encoder.predict(image[None])\n",
    "    \n",
    "    (distances,),(idx,) = nei_clf.kneighbors(code,n_neighbors=n_neighbors)\n",
    "    \n",
    "    return distances,images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:44.008658Z",
     "start_time": "2018-01-04T16:56:43.997658Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=3)\n",
    "    \n",
    "    plt.figure(figsize=[8,7])\n",
    "    plt.subplot(1,4,1)\n",
    "    show_image(image)\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1,4,i+2)\n",
    "        show_image(neighbors[i])\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько примеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:57:31.768260Z",
     "start_time": "2018-01-04T16:57:31.240174Z"
    }
   },
   "outputs": [],
   "source": [
    "# smiles\n",
    "show_similar(X_test[247])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:50.828404Z",
     "start_time": "2018-01-04T16:56:50.462822Z"
    }
   },
   "outputs": [],
   "source": [
    "# ethnicity\n",
    "show_similar(X_test[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:56:52.939288Z",
     "start_time": "2018-01-04T16:56:52.576097Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# glasses\n",
    "show_similar(X_test[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Быстрый морфинг изобржаений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем использовать коды изображений для составления нового путем их линейной комбинации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:57:58.378044Z",
     "start_time": "2018-01-04T16:57:58.145544Z"
    }
   },
   "outputs": [],
   "source": [
    "# restore trained encoder weights\n",
    "s = reset_tf_session()\n",
    "encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\n",
    "encoder.load_weights(\"encoder.h5\")\n",
    "decoder.load_weights(\"decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T16:58:03.396368Z",
     "start_time": "2018-01-04T16:58:00.359973Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    image1,image2 = X_test[np.random.randint(0,len(X_test),size=2)]\n",
    "\n",
    "    code1, code2 = encoder.predict(np.stack([image1, image2]))\n",
    "\n",
    "    plt.figure(figsize=[10,4])\n",
    "    for i,a in enumerate(np.linspace(0,1,num=7)):\n",
    "\n",
    "        output_code = code1*(1-a) + code2*(a)\n",
    "        output_image = decoder.predict(output_code[None])[0]\n",
    "\n",
    "        plt.subplot(1,7,i+1)\n",
    "        show_image(output_image)\n",
    "        plt.title(\"a=%.2f\"%a)\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
